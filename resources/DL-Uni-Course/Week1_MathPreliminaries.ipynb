{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YseAOWkjQbbt"
      },
      "source": [
        "# Deep Learning 2021-22 / Summer Semester \n",
        "## Week 1: Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhhv6pEzwBgx"
      },
      "source": [
        "Testing the potential of deep learning presents unique challenges because any single application brings together various disciplines. Applying deep learning requires simultaneously understanding (i) the motivations for casting a problem in a particular way; (ii) the mathematical form of a given model; (iii) the optimization algorithms for fitting the models to data; (iv) the statistical principles that tell us when we should expect our models to generalize to unseen data and practical methods for certifying that they have, in fact, generalized; and (v) the engineering techniques required to train models efficiently, navigating the pitfalls of numerical computing and getting the most out of available hardware. Teaching both the critical thinking skills required to formulate problems, the mathematics to solve them, and the software tools to implement those solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7lUr6mUQx1O"
      },
      "source": [
        "### Coding environment\n",
        "\n",
        "In this class we will mainly use Jupyter notebooks.  \n",
        "**What is a jupyter notebook?**\n",
        "*   open-source web application\n",
        "*   Supports over 40 programming languages, including Python, R, ...\n",
        "*   allows you to create and share documents containing live code, visualizations, equations, and narrative text (using Markdown)\n",
        "* Often used in Data Science (data cleaning, transformation, simulation, statistical modeling, visualization, machine learning)\n",
        "* A nice manual about all functionalities can be found [here](https://jupyter.brynmawr.edu/services/public/dblank/Jupyter%20Notebook%20Users%20Manual.ipynb)\n",
        "* Successor JupyterLab\n",
        "\n",
        "\n",
        "**What you need to work with jupyter notebooks**  \n",
        "There are multiple options:\n",
        "* Installing it with Anaconda or pip. For this you need to install Anaconda first (Note: we recommend to install the classic version, not miniconda). Here is the link to the [Anaconda resource.](https://docs.anaconda.com/anaconda/install/)\n",
        "* Here is a nice [blog](https://garywoodfine.com/set-up-anaconda-jupyter-notebook-tensorflow-for-deep-learning/), which explains you how to install jupyter with Anaconda and how to manage environments including python packages.\n",
        "*   [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true) (allows link sharing, uses a standard environment of python packages and versions) --> runs on a Google Server\n",
        "\n",
        "In this course, we will provide notebooks via Google Colab. If you don't have a google account you can download them and open it on your local machine. (Please note that we cannot provide support for version-problems etc.)\n",
        "\n",
        "Please set up your jupyter notebooks yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoAHOCsFfb-n"
      },
      "source": [
        "# Pytorch\n",
        "Ok. Back to this week's topic on Algebra and Calculus.   \n",
        "In Machine Learning we often deal with large matrices or arrays. For example when you want to analyze images, sound or sensor data. You can process this type of data with the Python modules numpy or pytorch. \n",
        "\n",
        "We will start by introducing the tensor, PyTorch's primary tool for storing and transforming numerical data. Tensors support asynchronous computation on CPU, GPU and provide support for automatic differentiation. Automatic differentiation in Numpy does not exist.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MML50_vEuBH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvwaQp3VwBRh"
      },
      "source": [
        "### Scalars, vectors, matrices and tensors\n",
        "\n",
        "Basic mathematical objects for storing data comprise scalars, vectors, matrices and tensors.\n",
        "\n",
        "\n",
        "*   Scalars are single numbers that are used to measure a quantity. I.e. 12.5 km or 20 degrees Celcius. Mathematical constants are also represented as scalar values.\n",
        "*  Vectors are a list of scalars\n",
        "* Tensors are n-dimensional arrays. Can be one-dimensional or more\n",
        "*  Matrices are two-dimensional vectors or arrays or 2-dimensional tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuhfEtdL2wLb",
        "outputId": "6040f9e7-1df6-4d4d-9fe3-579ac3fb3e45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#Scalar\n",
        "\n",
        "torch.tensor([np.array([3])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPuciQCH22eZ",
        "outputId": "384c07c3-138f-46a7-efd1-0a1b2ddb75ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
            "       dtype=torch.float64)\n",
            "tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "#Vectors\n",
        "\n",
        "x = torch.arange(12, dtype=torch.float64)\n",
        "print(x)\n",
        "\n",
        "y = torch.tensor(np.array([1,2,3]))\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ilw-W4We0fn",
        "outputId": "e2c7b6b6-fa44-48ef-f56f-a198a5cd658f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 1, 4, 3],\n",
            "        [1, 2, 3, 4],\n",
            "        [4, 3, 2, 1]]) \n",
            "\n",
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]]) \n",
            "\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "#Matrices\n",
        "\n",
        "M1 = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) \n",
        "print(M1,'\\n')\n",
        "\n",
        "M2 = torch.zeros((3, 4))\n",
        "print(M2,'\\n')\n",
        "\n",
        "M3 = torch.ones((3, 4)) \n",
        "print(M3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfyTwa813CHc",
        "outputId": "9900c2ba-421b-45de-ab27-9caae98222e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[8.7958e-35, 0.0000e+00, 7.0065e-44, 7.0065e-44],\n",
            "         [6.3058e-44, 6.7262e-44, 7.4269e-44, 6.3058e-44],\n",
            "         [6.7262e-44, 7.5670e-44, 1.1771e-43, 6.7262e-44]],\n",
            "\n",
            "        [[7.9874e-44, 8.1275e-44, 7.4269e-44, 7.0065e-44],\n",
            "         [8.1275e-44, 6.8664e-44, 7.1466e-44, 6.4460e-44],\n",
            "         [7.0065e-44, 7.8473e-44, 7.2868e-44, 7.1466e-44]]]) \n",
            "\n",
            "tensor([[[ 6.,  3.,  9.,  0.],\n",
            "         [ 4.,  6.,  9.,  1.],\n",
            "         [ 4.,  7., 10.,  3.]],\n",
            "\n",
            "        [[ 0.,  1., 10.,  0.],\n",
            "         [ 1.,  3.,  9.,  2.],\n",
            "         [ 4.,  9.,  0.,  8.]]])\n"
          ]
        }
      ],
      "source": [
        "#3D Tensor\n",
        "\n",
        "t1 = torch.Tensor(2, 3, 4)\n",
        "print(t1,'\\n') \n",
        "\n",
        "t2 = torch.Tensor(np.random.randint(12, size=(2,3,4))) \n",
        "print(t2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f3vVaAGlPGd"
      },
      "source": [
        "### Attributes of Tensors\n",
        "\n",
        "Tensors have certain attributes, for example a shape, number of dimensions and the datatype of their values. You can check for these attributes in the following way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBo_AQS8iV6B",
        "outputId": "dced146a-c0ee-4a67-9edc-102c9b8301a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape attribute:\n",
            "torch.Size([2, 3, 4]) \n",
            "\n",
            "Number of dimensions:\n",
            "3 \n",
            "\n",
            "Datatype of elements:\n",
            "torch.float32 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "X = torch.Tensor(2, 3, 4)\n",
        "\n",
        "print('Shape attribute:')\n",
        "print(X.shape, '\\n')\n",
        "\n",
        "print('Number of dimensions:')\n",
        "print(X.ndim, '\\n')\n",
        "\n",
        "print('Datatype of elements:')\n",
        "print(X.dtype, '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfuPf1GYlUTf"
      },
      "source": [
        "### Indexing to access single elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEpIAfapjBcX",
        "outputId": "1b288ec4-da4f-4551-f647-544ab4ce67dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 8.,  2.,  3.,  1.],\n",
            "         [ 5.,  0.,  4.,  4.],\n",
            "         [ 1., 10.,  8.,  8.]],\n",
            "\n",
            "        [[ 0.,  5.,  8., 11.],\n",
            "         [ 0.,  0.,  7.,  6.],\n",
            "         [ 9.,  2., 10.,  7.]]]) \n",
            "\n",
            "tensor(4.) \n",
            "\n",
            "tensor(8.)\n"
          ]
        }
      ],
      "source": [
        "X = torch.Tensor(np.random.randint(12, size=(2,3,4)))\n",
        "print(X, '\\n')\n",
        "\n",
        "print(X[0,1,2], '\\n')  #dim, row, col\n",
        "\n",
        "print(X[0,2,-1]) #last column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t9dMqq_maiu"
      },
      "source": [
        "### Slicing\n",
        "If we have matrices or higher-dimensional arrays, we can extract slices, meaning or multiple elements from them. Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the *slice* notation, marked by the colon (``:``) character.\n",
        "The slicing syntax follows that of the standard Python list; to access a slice of an array ``x``, use this:\n",
        "``` python\n",
        "x[start:stop:step]\n",
        "```\n",
        "If any of these are unspecified, they default to the values ``start=0``, ``stop=``*``size of dimension``*, ``step=1``.\n",
        "We'll take a look at accessing sub-arrays in one dimension and in multiple dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoNAoXKB0XTx",
        "outputId": "c42338e0-d02c-4715-f4d1-48a140f4c5af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.,  9.,  4.,  6.],\n",
            "         [ 7.,  4.,  3.,  1.],\n",
            "         [ 8.,  1.,  2., 10.]],\n",
            "\n",
            "        [[ 7.,  6.,  2.,  0.],\n",
            "         [ 0.,  5.,  2.,  0.],\n",
            "         [ 7.,  2.,  7., 11.]]]) \n",
            "\n",
            "\n",
            "tensor([[2., 9., 4., 6.]]) \n",
            "\n",
            "\n",
            "tensor([ 6.,  1., 10.]) \n",
            "\n",
            "\n",
            "tensor([[[7., 4., 3., 1.]],\n",
            "\n",
            "        [[0., 5., 2., 0.]]]) \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(X,'\\n\\n')\n",
        "\n",
        "print(X[0,0:1],'\\n\\n')      #Matrix in first dimension, first row\n",
        "\n",
        "print(X[0, :, -1],'\\n\\n')   #last column\n",
        " \n",
        "print(X[:, 1::2],'\\n\\n')    #Every other element, starting at index 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pLaUYlCpPRH"
      },
      "source": [
        "### Reshaping Arrays\n",
        "\n",
        "Changing the dimensions of a tensor is often used in Machine Learning. \n",
        "The most flexible way of doing this is with the ``reshape`` method. But make sure that you can fit all elements into the new reshaped tensor, otherwise you'll get an Error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIZqrBcJyEm_",
        "outputId": "a1e6db6c-471e-42d9-b87a-28fb8fc3efbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First, look at X: \n",
            " tensor([[[ 8.,  2.,  3.,  1.],\n",
            "         [ 5.,  0.,  4.,  4.],\n",
            "         [ 1., 10.,  8.,  8.]],\n",
            "\n",
            "        [[ 0.,  5.,  8., 11.],\n",
            "         [ 0.,  0.,  7.,  6.],\n",
            "         [ 9.,  2., 10.,  7.]]]) \n",
            "\n",
            "X's shape is: torch.Size([2, 3, 4]) \n",
            "\n",
            "Reshaped X into Xr with size 6x4 becomes: \n",
            " tensor([[ 8.,  2.,  3.,  1.],\n",
            "        [ 5.,  0.,  4.,  4.],\n",
            "        [ 1., 10.,  8.,  8.],\n",
            "        [ 0.,  5.,  8., 11.],\n",
            "        [ 0.,  0.,  7.,  6.],\n",
            "        [ 9.,  2., 10.,  7.]]) \n",
            "\n",
            "Now reshaping back into 3D but different size: \n",
            " tensor([[[ 8.,  2.,  3.],\n",
            "         [ 1.,  5.,  0.],\n",
            "         [ 4.,  4.,  1.],\n",
            "         [10.,  8.,  8.]],\n",
            "\n",
            "        [[ 0.,  5.,  8.],\n",
            "         [11.,  0.,  0.],\n",
            "         [ 7.,  6.,  9.],\n",
            "         [ 2., 10.,  7.]]]) \n",
            "\n",
            "torch.Size([3]) \n",
            "\n",
            "tensor([[1., 2., 3.]]) \n",
            "\n",
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Let't turn our 3 dimensional tensor with the dimensions (2,3,4) into a 2-dimensional (6,4) tensors\n",
        "print('First, look at X: \\n', X, '\\n')\n",
        "\n",
        "print( \"X's shape is:\", X.shape , '\\n') #2*3*4 = 24 elements\n",
        "Xr = X.reshape((6, 4))\n",
        "print('Reshaped X into Xr with size 6x4 becomes: \\n', Xr, '\\n')\n",
        "\n",
        "#Another example from 2D into 3D\n",
        "print(\"Reshaping back into 3D but into a different sized tensor: \\n\", Xr.reshape((2,4,3)), '\\n' )\n",
        "\n",
        "# Another common reshaping pattern is the conversion of a one-dimensional array/tensor into a two-dimensional row or column matrix.\n",
        "X1 = torch.Tensor([1, 2, 3])\n",
        "print(X1, '\\n', X1.shape, '\\n')\n",
        "\n",
        "#row vector via reshape\n",
        "print(X1.reshape((1, 3)) , '\\n')\n",
        "\n",
        "# column vector via reshape\n",
        "print(X1.reshape((3, 1)) , '\\n') # 2D (3,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dRs-co0rtYL"
      },
      "source": [
        "### Arithmetics\n",
        "\n",
        "You can apply arithmetis on entire tensors and the function will be applied on each element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWh_y1oHjoGv",
        "outputId": "ccec5d8f-b21b-43eb-d765-55cf3f19647a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1:  tensor([1., 2., 3.]) \n",
            "\n",
            "Multiplied by 2:  tensor([2., 4., 6.]) \n",
            "\n",
            "Divided by 2:  tensor([0.5000, 1.0000, 1.5000]) \n",
            "\n",
            "Modulo 2:  tensor([1., 0., 1.]) \n",
            "\n",
            "X1 // 2 = tensor([0., 1., 1.]) \n",
            "\n",
            "e^X1 = tensor([ 2.7183,  7.3891, 20.0855]) \n",
            "\n",
            "X1^3 = tensor([ 3.,  9., 27.]) \n",
            "\n",
            "log(X1) = tensor([0.0000, 0.6931, 1.0986]) \n",
            "\n",
            "log2(X1) = tensor([0.0000, 1.0000, 1.5850]) \n",
            "\n",
            "log10(X1) = tensor([0.0000, 0.3010, 0.4771]) \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  import sys\n"
          ]
        }
      ],
      "source": [
        "print('X1: ', X1, '\\n')\n",
        "\n",
        "#Multiplication and Division\n",
        "print(\"Multiplied by 2: \",X1*2, '\\n') #Multiplication\n",
        "print(\"Divided by 2: \",X1/2, '\\n') #Division\n",
        "print(\"Modulo 2: \",X1%2, '\\n') #Modulo\n",
        "print(\"X1 // 2 =\", X1 // 2, '\\n') # integer (floor) division\n",
        "\n",
        "#Exponents\n",
        "print(\"e^X1 =\",torch.exp(X1), '\\n')  #e^X1\n",
        "print(\"X1^3 =\",torch.pow(3, X1), '\\n') #X1^3\n",
        "\n",
        "#Logarithms\n",
        "print(\"log(X1) =\",torch.log(X1), '\\n')\n",
        "print(\"log2(X1) =\",torch.log2(X1), '\\n')\n",
        "print(\"log10(X1) =\",torch.log10(X1), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-ZHhr6D_i_9"
      },
      "source": [
        "### Matrix Operations \n",
        "Typical operations on tensors are transpositions, obtaining the determinant, the inverse and the norm. These can be coded out as followed:  \n",
        "\n",
        "\n",
        "Operation  | Math | Numpy\n",
        "-------------------|------------------|--------  \n",
        "Transposition | $A^T$ |  A.t(), A.transpose()\n",
        "Determinant | $|A|$ | torch.det(A)\n",
        "Inverse | $A^{-1}$| torch.inverse(A)\n",
        "Norm | $\\|A\\|$ | torch.norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGim9SnUzy-y"
      },
      "source": [
        "#### Transpose of a matrix\n",
        "The transpose flips a matrix over its diagonale, meaning that the row-values are depicted in columns.\n",
        "\n",
        "In pytorch, you can simply apply the ``t()``-function to a matrix or use the ``transpose()`` function, if you have multiple dimensions. \n",
        "Pytorch's transpose function, requires that you specify the dimension, which you want to transpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB3HpkoUz_Fj",
        "outputId": "bfc6f1b7-5aea-4fdb-9a4e-f430854f84a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]]) \n",
            "\n",
            "Transpose: \n",
            "tensor([[0, 3, 6],\n",
            "        [1, 4, 7],\n",
            "        [2, 5, 8]]) \n",
            "\n",
            "tensor([[[ 6.,  4.,  8.,  7.],\n",
            "         [ 4., 11.,  8.,  6.],\n",
            "         [ 0.,  2.,  9.,  4.]],\n",
            "\n",
            "        [[10.,  7.,  5.,  0.],\n",
            "         [ 4.,  9.,  0.,  5.],\n",
            "         [ 0.,  2., 11.,  3.]]]) \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 6.,  4.,  8.,  7.],\n",
              "         [10.,  7.,  5.,  0.]],\n",
              "\n",
              "        [[ 4., 11.,  8.,  6.],\n",
              "         [ 4.,  9.,  0.,  5.]],\n",
              "\n",
              "        [[ 0.,  2.,  9.,  4.],\n",
              "         [ 0.,  2., 11.,  3.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#Transpose matrices with t()\n",
        "A = torch.arange(9).reshape(3,3)\n",
        "print(A, '\\n')\n",
        "\n",
        "print('Transpose: ')\n",
        "print( A.t(), '\\n' )\n",
        "\n",
        "#For 3-dimensional (or more) tensors you can use transpose()\n",
        "A3 = torch.Tensor(np.random.randint(12, size=(2,3,4)))\n",
        "print(A3, '\\n')\n",
        "torch.transpose(A3, 0, 1) #Indicate the dimesions to be transposed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkFf5VANKX3t"
      },
      "source": [
        "#### The determinant of a matrix\n",
        "The determinant is a scalar value, which characterizes the properties of a matrix. The determinant of a matrix A is denoted as $|A|$. \n",
        "\n",
        "The determinant of a 2x2 matrix can be defined as:  \n",
        "$|A| = \n",
        "\\begin{vmatrix}\n",
        "a & b \\\\ \n",
        "c & d \\\\\n",
        "\\end{vmatrix} = ad-bc$   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31ZFRn7XKeCP",
        "outputId": "19b1fc01-5606-4d4d-a582-fd13fddf5ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1.],\n",
            "        [2., 3.]], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-2., dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "A = torch.arange(4, dtype=float).reshape(2,2)\n",
        "print(A)\n",
        "torch.det(A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05sotstHz10Z"
      },
      "source": [
        "#### Inverse of a matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrfNnz_TOcd3"
      },
      "source": [
        "$A^{-1} = \n",
        "\\frac{1}{det(A)}*\n",
        "\\begin{vmatrix}\n",
        "d & -b \\\\ \n",
        "-c & a \\\\\n",
        "\\end{vmatrix}$   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWMU6OJIpVWg",
        "outputId": "201a0379-f99c-4c22-9fea-5ed02a2d3cca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.5000,  0.5000],\n",
              "        [ 1.0000,  0.0000]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "torch.inverse(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6coxslUwBg8"
      },
      "outputs": [],
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8WH6VGezp0O"
      },
      "source": [
        "### Norms\n",
        "\n",
        "Informally, a Norm tells us how big a vector, matrix or tensor is.\n",
        "\n",
        "There are different classes of so called p-norms. P can be a number >1. Special Norms are:\n",
        "*   P=1: Sum of all absolute values in a tensor: $\\|x\\|_1 = \\Sigma^n_{i=1} |x_i|$\n",
        "*   P=2: The Euklidean Norm (or: $\\ell_2$-norm) is calculated as $\\|x\\|_2 = \\sqrt{x_1^2 + \\cdots + x_n^2}$ for vectors and for matrices it is: $\\sigma_1(A)$\n",
        "* Frobenius Norm: Only exists for matrices and is defined as $\\sqrt{\\sum_{i,j} a_{ij}^2}$ or $\\sqrt{\\sum_{i}\\sigma_i^2(A)}$, where $\\sigma_i$ is the ith singular value of A\n",
        "\n",
        "In Machine Learning we often work with the $\\ell_1$- and $\\ell_2$ norms to improve our model (we will get there later in the semester :-) )\n",
        "\n",
        "To calculate the norm, we can call ``torch.linalg.norm()`` and specify *p* in the 'ord' parameter ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6S8zb2S5on0",
        "outputId": "23b51c3f-d317-4256-d2af-ab0970ad3913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.]) \n",
            "\n",
            "tensor(6.) \n",
            "\n",
            "tensor(3.7417) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(X1,'\\n')\n",
        "\n",
        "#L1-Norm\n",
        "print( torch.linalg.norm(X1, ord=1) , '\\n')\n",
        "\n",
        "#L2-Norm\n",
        "print( torch.linalg.norm(X1, ord=2) , '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Ss7xuvFLuT"
      },
      "source": [
        "### Multiplication and Dot product\n",
        "Element-wise multiplications (often called Haddamart-Product) multiply values at the same position with each other. \n",
        "\n",
        "However, one of the most fundamental operations is the dot product.  \n",
        "Given two vectors $x,y \\in \\mathbb{R}^d$, their dot product $x^Ty$ is a sum over the products of the elements at the same position: $x^Ty=\\Sigma^d_{i=1}x_iy_i$.\n",
        "\n",
        "Dot products are useful in a wide range of contexts. For example, given a set of weights $\\mathbf{w}$, the weighted sum of some values ${u}$ could be expressed as the dot product $\\mathbf{u}^T \\mathbf{w}$. When the weights are non-negative and sum to one $\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)$, the dot product expresses a *weighted average*. When two vectors each have length one (we will discuss what *length* means below in the section on norms), dot products can also capture the cosine of the angle between them.\n",
        "\n",
        "Here is an overview of how to implement them.\n",
        "\n",
        "*   Element-wise-multiplication simply with `*`\n",
        "*   Vector-vector dot-product with `torch.dot()`\n",
        "* Matrix-vector dot-product with `torch.mv()`\n",
        "* Matrix-Matrix dot-product with `torch.mm()`\n",
        "\n",
        "\n",
        "Below some examples. You can also see if you can reproduce them on paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-qjm5rK-Uss",
        "outputId": "382d931f-036c-414b-fa5a-3d60045fc00a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 4., 6.])\n",
            "tensor([[ 2.,  4.],\n",
            "        [12., 20.]])\n",
            "tensor([[ 6.,  8.],\n",
            "        [ 9., 16.]])\n",
            "tensor(12.)\n",
            "The matrix: \n",
            " tensor([[2., 2.],\n",
            "        [3., 4.]])\n",
            "The vector: \n",
            " tensor([2., 2.])\n",
            "The product of both: \n",
            " tensor([ 8., 14.])\n",
            "tensor([[10., 14.],\n",
            "        [19., 26.]])\n"
          ]
        }
      ],
      "source": [
        "#Element-wise multiplication of two vectors\n",
        "x1 = torch.Tensor(np.array([2,2,2]))\n",
        "x2 = torch.Tensor(np.array([1,2,3]))\n",
        "print(x1*x2)\n",
        "\n",
        "#Element-wise Matrix-Matrix multiplication\n",
        "M1 = torch.Tensor(np.array([[2, 2], [3, 4]])) #[first row], [second row]\n",
        "M2 = torch.Tensor(np.array([[1, 2], [4, 5]]))\n",
        "print(M1*M2)\n",
        "\n",
        "#Element-wise matrix vector products\n",
        "a = torch.Tensor(np.array([3,4]))\n",
        "print(a*M1)\n",
        "\n",
        "\n",
        "# Vector-vector dot product results in a scalar\n",
        "print(torch.dot(x1, x2))\n",
        "\n",
        "#A matrix-vector dot-product\n",
        "print('The matrix: \\n', M1)\n",
        "\n",
        "vec = torch.Tensor(np.array([2,2]))\n",
        "print('The vector: \\n', vec)\n",
        "\n",
        "prod = torch.mv(M1, vec)\n",
        "print('The product of both: \\n', prod)\n",
        "\n",
        "print(torch.mm(M1,M2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgLExeqFwBg-"
      },
      "source": [
        "In just this section, we have taught you all the linear algebra that you will need to understand a remarkable chunk of modern deep learning. There is a lot more to linear algebra and a lot of that mathematics is useful for machine learning. For example, matrices can be decomposed into factors, and these decompositions can reveal low-dimensional structure in real-world datasets. There are entire subfields of machine learning that focus on using matrix decompositions and their generalizations to high-order tensors to discover structure in datasets and solve prediction problems. But this book focuses on deep learning. And we believe you will be much more inclined to learn more mathematics once you have gotten your hands dirty deploying useful machine learning models on real datasets. So while we reserve the right to introduce more mathematics much later on, we will wrap up this section here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I5uFY-ZwBg-"
      },
      "source": [
        "### Why do we need Calculus? \n",
        "\n",
        "In deep learning, we train models, updating them successively so that they get better and better as they see more and more data. Usually, getting better means minimizing a loss function, a score that answers the question “how bad is our model?” This question is more subtle than it appears. Ultimately, what we really care about is producing a model that performs well on data that we have never seen before. But we can only fit the model to data that we can actually see. Thus we can decompose the task of fitting models into two key concerns: (i) optimization: the process of fitting our models to observed data; (ii) generalization: the mathematical principles and practitioners’ wisdom that guide as to how to produce models whose validity extends beyond the exact set of data examples used to train them.\n",
        "\n",
        "To help you understand optimization problems and methods in later chapters, here we give a very brief primer on differential calculus that is commonly used in deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjLpNC88SeW"
      },
      "source": [
        "### Automatic Differentiation\n",
        "\n",
        "Pytorch is amazing because it comes with the 'autograd' package. The autograd package can automatically calculate derivatives. This would not be possible with numpy. \n",
        "\n",
        "You will learn how this works in the next lectures. For now we just want to show you how pytorch can calulate derivatives.\n",
        "\n",
        "**A simple example**  \n",
        "Let's find the derivative of the following function:  \n",
        "$f(x) = 5x^4 + 3x^3 + 7x^2 + 9x -5$\n",
        "\n",
        "If we write this out by hand, the first derivative is:  \n",
        "$f'(x) = 20x^3 + 9x^2 + 14x + 9$\n",
        "\n",
        "Now, we want to find the value of this derivative where x=2.  \n",
        "$f'(2) = 160 + 36 + 28 + 9 = 233$\n",
        "\n",
        "To start, let's create the variable `x` and assign it to an initial value.\n",
        "Once we compute the gradient of ``y`` with respect to ``x``. We can tell pytorch that we want to store a gradient of our tensor by the ``requires_grad=True`` keyword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUFRZEkN9ZtR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj1EB7pGFR_D",
        "outputId": "373f134b-88cc-4cb1-adb9-1af1042ac220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x= tensor([2.], requires_grad=True)\n",
            "y= tensor([145.], grad_fn=<SubBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = torch.autograd.Variable(torch.Tensor([2]),requires_grad=True) #Defines the value at which we want to compute the derivative\n",
        "y = 5*x**4 + 3*x**3 + 7*x**2 + 9*x - 5 #Defines the function we want to compute the derivative of \n",
        "print('x=', x)\n",
        "print('y=', y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUsoURy2H_yC",
        "outputId": "96b9be25-18d7-4d4b-c818-d7ec3dea9432"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([233.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "y.backward() #This function calculates the gradient or first derivative of y w.r.t x=2\n",
        "x.grad #Outputs the derivative value stored at x=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AKz9EgaPg2A"
      },
      "source": [
        "### Data Import"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
        "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
        "with open(data_file, 'w') as f:\n",
        "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
        "    f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
        "    f.write('2,NA,106000\\n')\n",
        "    f.write('4,NA,178100\\n')\n",
        "    f.write('NA,NA,140000\\n')"
      ],
      "metadata": {
        "id": "kIqtjUuF4WbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbJGvLkQwBhH",
        "outputId": "362270c8-4bd6-46d7-9779-b1203980ce8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms Alley   Price\n",
            "0       NaN  Pave  127500\n",
            "1       2.0   NaN  106000\n",
            "2       4.0   NaN  178100\n",
            "3       NaN   NaN  140000 \n",
            "\n",
            "   NumRooms Alley\n",
            "0       3.0  Pave\n",
            "1       2.0   NaN\n",
            "2       4.0   NaN\n",
            "3       3.0   NaN \n",
            "\n",
            "   NumRooms  Alley_Pave  Alley_nan\n",
            "0       3.0           1          0\n",
            "1       2.0           0          1\n",
            "2       4.0           0          1\n",
            "3       3.0           0          1 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[3., 1., 0.],\n",
              "         [2., 0., 1.],\n",
              "         [4., 0., 1.],\n",
              "         [3., 0., 1.]], dtype=torch.float64),\n",
              " tensor([127500, 106000, 178100, 140000]))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "data = pd.read_csv(data_file)\n",
        "print(data,'\\n')\n",
        "\n",
        "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
        "inputs = inputs.fillna(inputs.mean())\n",
        "print(inputs,'\\n')\n",
        "\n",
        "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
        "print(inputs,'\\n')\n",
        "\n",
        "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
        "X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0Ui863qwBhH"
      },
      "source": [
        "### Finding All the Functions and Classes in a Module\n",
        "In order to know which functions and classes can be called in a module, we invoke the dir function. For instance, we can query all properties in the module for generating random numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI8o_jQtwBhH",
        "outputId": "70d5eea0-8799-4682-a293-677a8a7c8aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n"
          ]
        }
      ],
      "source": [
        "print(dir(torch.distributions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xla0fvWBwBhH"
      },
      "source": [
        "Generally, we can ignore functions that start and end with __ (special objects in Python) or functions that start with a single _(usually internal functions). Based on the remaining function or attribute names, we might hazard a guess that this module offers various methods for generating random numbers, including sampling from the uniform distribution (uniform), normal distribution (normal), and multinomial distribution (multinomial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Z5ZjJAwBhH"
      },
      "source": [
        "### Finding the Usage of Specific Functions and Classes\n",
        "For more specific instructions on how to use a given function or class, we can invoke the help function. As an example, let us explore the usage instructions for tensors’ ones function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4w_R-b1wBhI",
        "outputId": "498a4e03-f6d9-4e17-c5a8-4400bd5b3626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on built-in function ones:\n",
            "\n",
            "ones(...)\n",
            "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
            "    \n",
            "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
            "    by the variable argument :attr:`size`.\n",
            "    \n",
            "    Args:\n",
            "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
            "            Can be a variable number of arguments or a collection like a list or tuple.\n",
            "    \n",
            "    Keyword arguments:\n",
            "        out (Tensor, optional): the output tensor.\n",
            "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
            "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
            "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
            "            Default: ``torch.strided``.\n",
            "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            "            Default: if ``None``, uses the current device for the default tensor type\n",
            "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
            "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
            "        requires_grad (bool, optional): If autograd should record operations on the\n",
            "            returned tensor. Default: ``False``.\n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> torch.ones(2, 3)\n",
            "        tensor([[ 1.,  1.,  1.],\n",
            "                [ 1.,  1.,  1.]])\n",
            "    \n",
            "        >>> torch.ones(5)\n",
            "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(torch.ones)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZpXMwr_wBhI"
      },
      "source": [
        "In the Jupyter notebook, we can use ? to display the document in another window. For example, list? will create content that is almost identical to help(list), displaying it in a new browser window. In addition, if we use two question marks, such as list??, the Python code implementing the function will also be displayed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Week1_MathPreliminaries.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
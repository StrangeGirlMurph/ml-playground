{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week6_notebook_extra.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGNpfPn7Ck5Q"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Weight Decay\n",
        "\n",
        "Now that we have characterized the problem of overfitting\n",
        "and motivated the need for capacity control,\n",
        "we can begin discussing some of the popular techniques\n",
        "used to these ends in practice.\n",
        "Recall that we can always mitigate overfitting\n",
        "by going out and collecting more training data,\n",
        "that can be costly and time consuming,\n",
        "typically making it impossible in the short run.\n",
        "For now, let's assume that we have already obtained\n",
        "as much high-quality data as our resources permit\n",
        "and focus on techniques aimed at limiting the capacity\n",
        "of the function classes under consideration.\n",
        "\n",
        "In our toy example,\n",
        "we saw that we could control the complexity of a polynomial\n",
        "by adjusting its degree.\n",
        "However, most of machine learning\n",
        "does not consist of polynomial curve fitting.\n",
        "And moreover, even when we focus on polynomial regression,\n",
        "when we deal with high-dimensional data,\n",
        "manipulating model capacity by tweaking the degree $d$ is problematic.\n",
        "To see why, note that for multivariate data\n",
        "we must generalize the concept of polynomials\n",
        "to include *monomials*, which are simply\n",
        "products of powers of variables.\n",
        "For example, $x_1^2 x_2$, and $x_3 x_5^2$ are both monomials of degree $3$.\n",
        "The number of such terms with a given degree $d$\n",
        "blows up as a function of the degree $d$.\n",
        "\n",
        "Concretely, for vectors of dimensionality $D$,\n",
        "the number of monomials of a given degree $d$ is ${D -1 + d} \\choose {D-1}$.\n",
        "Hence, a small change in degree, even from say $1$ to $2$ or $2$ to $3$\n",
        "would entail a massive blowup in the complexity of our model.\n",
        "Thus, tweaking the degree is too blunt a hammer.\n",
        "Instead, we need a more fine-grained tool\n",
        "for adjusting function complexity.\n",
        "\n",
        "## Squared Norm Regularization\n",
        "\n",
        "*Weight decay* (commonly called *L2* regularization),\n",
        "might be the most widely-used technique\n",
        "for regularizing parametric machine learning models.\n",
        "The basic intuition behind weight decay is\n",
        "the notion that among all functions $f$,\n",
        "the function $f = 0$ is the simplest.\n",
        "Intuitively, we can then measure functions by their proximity to zero.\n",
        "But how precisely should we measure\n",
        "the distance between a function and zero?\n",
        "There is no single right answer.\n",
        "In fact, entire branches of mathematics,\n",
        "e.g. in functional analysis and the theory of Banach spaces\n",
        "are devoted to answering this issue.\n",
        "\n",
        "For our present purposes, a very simple interpretation will suffice:\n",
        "We will consider a linear function\n",
        "$f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$\n",
        "to be simple if its weight vector is small.\n",
        "We can measure this via $||\\mathbf{w}||^2$.\n",
        "One way of keeping the weight vector small\n",
        "is to add its norm as a penalty term\n",
        "to the problem of minimizing the loss.\n",
        "Thus we replace our original objective,\n",
        "*minimize the prediction error on the training labels*,\n",
        "with new objective,\n",
        "*minimize the sum of the prediction error and the penalty term*.\n",
        "Now, if the weight vector becomes too large,\n",
        "our learning algorithm will find more profit in\n",
        "minimizing the norm $|| \\mathbf{w} ||^2$\n",
        "versus minimizing the training error.\n",
        "That's exactly what we want.\n",
        "To illustrate things in code, let's revive our previous example\n",
        "from linear regression.\n",
        "There, our loss was given by\n",
        "\n",
        "$$l(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
        "\n",
        "Recall that $\\mathbf{x}^{(i)}$ are the observations,\n",
        "$y^{(i)}$ are labels, and $(\\mathbf{w}, b)$\n",
        "are the weight and bias parameters respectively.\n",
        "To arrive at a new loss function\n",
        "that penalizes the size of the weight vector,\n",
        "we need to add $||\\mathbf{w}||^2$, but how much should we add?\n",
        "To address this, we need to add a new hyperparameter,\n",
        "that we will call the *regularization constant* and denote by $\\lambda$:\n",
        "\n",
        "$$l(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\boldsymbol{w}\\|^2$$\n",
        "\n",
        "This non-negative parameter $\\lambda \\geq 0$\n",
        "governs the amount of regularization.\n",
        "For $\\lambda = 0$, we recover our original loss function,\n",
        "whereas for $\\lambda > 0$ we ensure that $\\mathbf{w}$ cannot grow too large. The astute reader might wonder why we are squaring\n",
        "the norm of the weight vector.\n",
        "We do this for two reasons.\n",
        "First, we do it for computational convenience.\n",
        "By squaring the L2 norm, we remove the square root,\n",
        "leaving the sum of squares of each component of the weight vector.\n",
        "This is convenient because it is easy to compute derivatives of a sum of terms (the sum of derivatives equals the derivative of the sum).\n",
        "\n",
        "Moreover, you might ask, why the L2 norm in the first place and not the L1 norm, or some other distance function.\n",
        "In fact, several other choices are valid\n",
        "and are popular throughout statistics.\n",
        "While L2-regularized linear models constitute\n",
        "the classic *ridge regression* algorithm\n",
        "L1-regularizaed linear regression\n",
        "is a similarly fundamental model in statistics\n",
        "popularly known as *lasso regression*.\n",
        "\n",
        "One mathematical reason for working with the L2 norm and not some other norm,\n",
        "is that it penalizes large components of the weight vector\n",
        "much more than it penalizes small ones.\n",
        "This encourages our learning algorithm to discover models\n",
        "which distribute their weight across a larger number of features,\n",
        "which might make them more robust in practice\n",
        "since they do not depend precariously on a single feature.\n",
        "The stochastic gradient descent updates for L2-regularied regression\n",
        "are as follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "w & \\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "As before, we update $\\mathbf{w}$ based on the amount\n",
        "by which our estimate differs from the observation.\n",
        "However, we also shrink the size of $\\mathbf{w}$ towards $0$.\n",
        "That's why the method is sometimes called \"weight decay\":\n",
        "because the penalty term literally causes our optimization algorthm\n",
        "to *decay* the magnitude of the weight at each step of training.\n",
        "This is more convenient than having to pick\n",
        "the number of parameters as we did for polynomials.\n",
        "In particular, we now have a continuous mechanism\n",
        "for adjusting the complexity of $f$.\n",
        "Small values of $\\lambda$ correspond to unconstrained $\\mathbf{w}$,\n",
        "whereas large values of $\\lambda$ constrain $\\mathbf{w}$ considerably.\n",
        "Since we don't want to have large bias terms either,\n",
        "we often add $b^2$ as a penalty, too.\n",
        "\n",
        "## High-dimensional Linear Regression\n",
        "\n",
        "For high-dimensional regression it is difficult\n",
        "to pick the 'right' dimensions to omit.\n",
        "Weight-decay regularization is a much more convenient alternative.\n",
        "We will illustrate this below.\n",
        "First, we will generate some synthetic data as before\n",
        "\n",
        "$$y = 0.05 + \\sum_{i = 1}^d 0.01 x_i + \\epsilon \\text{ where }\n",
        "\\epsilon \\sim \\mathcal{N}(0, 0.01)$$\n",
        "\n",
        "representing our label as a linear function of our inputs,\n",
        "corrupted by Gaussian noise with zero mean and variance 0.01.\n",
        "To observe the effects of overfitting more easily,\n",
        "we can make our problem high-dimensional,\n",
        "setting the data dimension to $d = 200$\n",
        "and working with a relatively small number of training examples—here we'll set the sample size to 20:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "2"
        },
        "id": "wWZsm9VYCk5R"
      },
      "source": [
        "import torch\n",
        "import sys\n",
        "from IPython import display\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "%config InlineBackend.figure_formats = ['svg']\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
        "true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n",
        "\n",
        "def synthetic_data(w, b, num_examples): \n",
        "    \"\"\"Generate y = X w + b + noise.\"\"\"\n",
        "    X = torch.zeros(size=(num_examples, len(w))).normal_()\n",
        "    y = torch.matmul(X, w) + b\n",
        "    y += torch.zeros(size=y.shape).normal_(std=0.01)\n",
        "    return X, y\n",
        "\n",
        "def load_array(data_arrays, batch_size, is_train=True):  \n",
        "    \"\"\"Construct a PyTorch data loader\"\"\"\n",
        "    dataset = data.TensorDataset(*data_arrays)\n",
        "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "train_data = synthetic_data(true_w, true_b, n_train)\n",
        "train_iter = load_array(train_data, batch_size)\n",
        "test_data = synthetic_data(true_w, true_b, n_test)\n",
        "test_iter = load_array(test_data, batch_size, is_train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI4A4dGFCk5U"
      },
      "source": [
        "## Implementation from Scratch\n",
        "\n",
        "Next, we will show how to implement weight decay from scratch.\n",
        "All we have to do here is to add the squared $\\ell_2$ penalty\n",
        "as an additional loss term added to the original target function.\n",
        "The squared norm penalty derives its name from the fact\n",
        "that we are adding the second power $\\sum_i w_i^2$.\n",
        "The $\\ell_2$ is just one among an infinite class of norms call p-norms,\n",
        "many of which you might encounter in the future.\n",
        "In general, for some number $p$, the $\\ell_p$ norm is defined as\n",
        "\n",
        "$$\\|\\mathbf{w}\\|_p^p := \\sum_{i=1}^d |w_i|^p$$\n",
        "\n",
        "### Initialize Model Parameters\n",
        "\n",
        "First, we'll define a function to randomly initialize our model parameters and set `requires_grad`as `True` on each to allocate memory for the gradients we will calculate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "5"
        },
        "id": "bC5u23DACk5U"
      },
      "source": [
        "def init_params():\n",
        "    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n",
        "    b = torch.zeros(1, requires_grad=True)\n",
        "    return [w, b]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g758VGqXCk5X"
      },
      "source": [
        "### Define $\\ell_2$ Norm Penalty\n",
        "\n",
        "Perhaps the most convenient way to implement this penalty\n",
        "is to square all terms in place and summ them up.\n",
        "We divide by $2$ by convention\n",
        "(when we take the derivative of a quadratic function,\n",
        "the $2$ and $1/2$ cancel out, ensuring that the expression\n",
        "for the update looks nice and simple)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "6"
        },
        "id": "vVL1ZuFLCk5X"
      },
      "source": [
        "def l2_penalty(w):\n",
        "    return torch.sum(w.pow(2)) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDK9bG1xCk5Z"
      },
      "source": [
        "### Define Training and Testing\n",
        "\n",
        "The following code defines how to train and test the model\n",
        "separately on the training data set and the test data set.\n",
        "Unlike the previous sections, here, the $\\ell_2$ norm penalty term\n",
        "is added when calculating the final loss function.\n",
        "The linear network and the squared loss\n",
        "haven't changed since the previous chapter,\n",
        "so we'll just import them via `linreg` and `squared_loss`\n",
        "to reduce clutter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjQpzBS8HhaM"
      },
      "source": [
        "## A set of helper functions and classes \n",
        "def linreg(X, w, b):\n",
        "     return torch.matmul(X, w) + b\n",
        "\n",
        "def squared_loss(y_hat, y):\n",
        "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
        "\n",
        "def sgd(params, lr, batch_size):\n",
        "    for param in params:\n",
        "        param.data += -lr * param.grad.data / batch_size\n",
        "\n",
        "def evaluate_loss(net, data_iter, loss):  \n",
        "    \"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\n",
        "    metric = Accumulator(2)  # sum_loss, num_examples\n",
        "    for X, y in data_iter:\n",
        "        l = loss(net(X), y.reshape(-1, 1))\n",
        "        if l.nelement() != 1:\n",
        "            metric.add(l.sum(), y.numpy().size)\n",
        "        else:\n",
        "            metric.add(l*len(y), y.numpy().size)\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None, legend=None, figsize=(3.5, 2.5)):    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.semilogy(x_vals, y_vals)\n",
        "    if x2_vals and y2_vals:\n",
        "        plt.semilogy(x2_vals, y2_vals, linestyle=':')\n",
        "        plt.legend(legend)\n",
        "        \n",
        "class Animator: \n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear', fmts=None,\n",
        "                 nrows=1, ncols=1, figsize=(3.5, 2.5)):\n",
        "        \"\"\"Incrementally plot multiple lines.\"\"\"\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        \n",
        "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [self.axes, ]\n",
        "        # Use a lambda to capture arguments\n",
        "        self.config_axes = lambda: self.set_axes(\n",
        "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def set_axes(self, axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):  \n",
        "        \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "        axes.set_xlabel(xlabel)\n",
        "        axes.set_ylabel(ylabel)\n",
        "        axes.set_xscale(xscale)\n",
        "        axes.set_yscale(yscale)\n",
        "        axes.set_xlim(xlim)\n",
        "        axes.set_ylim(ylim)\n",
        "        if legend:\n",
        "            axes.legend(legend)\n",
        "        axes.grid()\n",
        "\n",
        "    def add(self, x, y):\n",
        "        \"\"\"Add multiple data points into the figure.\"\"\"\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        if not self.fmts:\n",
        "            self.fmts = ['-'] * n\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "class Accumulator:  \n",
        "    \"\"\"Sum a list of numbers over time.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a+float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "id": "ghAfNWrsCk5a"
      },
      "source": [
        "def train(lambd):\n",
        "    w, b = init_params()\n",
        "    net, loss = lambda X: linreg(X, w, b), squared_loss\n",
        "    num_epochs, lr = 100, 0.003\n",
        "    animator = Animator(xlabel='epochs', ylabel='loss', yscale='log',\n",
        "                            xlim=[1, num_epochs], legend=['train', 'test'])\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        for X, y in train_iter:\n",
        "            with torch.enable_grad():\n",
        "                # The L2 norm penalty term has been added, and broadcasting\n",
        "                # makes l2_penalty(w) a vector whose length is batch_size\n",
        "                l = loss(net(X), y) + lambd * l2_penalty(w)\n",
        "            l.sum().backward()\n",
        "            sgd([w, b], lr, batch_size)\n",
        "        if epoch % 5 == 0:\n",
        "            animator.add(epoch, (evaluate_loss(net, train_iter, loss),\n",
        "                                 evaluate_loss(net, test_iter, loss)))\n",
        "    print('l1 norm of w:', torch.norm(w).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD-_M5n5Ck5c"
      },
      "source": [
        "### Training without Regularization\n",
        "\n",
        "Next, let's train and test the high-dimensional linear regression model.\n",
        "When `lambd = 0` we do not use weight decay.\n",
        "As a result, while the training error decreases, the test error does not.\n",
        "This is a perfect example of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "8"
        },
        "id": "EsNLzMc_Ck5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "0dee1fa6-d905-4aad-988f-29798f701422"
      },
      "source": [
        "train(lambd=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l1 norm of w: 32.027957916259766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 257.521875 180.65625\" width=\"257.521875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 257.521875 180.65625 \nL 257.521875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 143.1 \nL 240.778125 143.1 \nL 240.778125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pf1021bcace)\" d=\"M 82.959943 143.1 \nL 82.959943 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mf7740f37c5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"82.959943\" xlink:href=\"#mf7740f37c5\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(76.597443 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pf1021bcace)\" d=\"M 122.414489 143.1 \nL 122.414489 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.414489\" xlink:href=\"#mf7740f37c5\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(116.051989 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pf1021bcace)\" d=\"M 161.869034 143.1 \nL 161.869034 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.869034\" xlink:href=\"#mf7740f37c5\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(155.506534 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pf1021bcace)\" d=\"M 201.32358 143.1 \nL 201.32358 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.32358\" xlink:href=\"#mf7740f37c5\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(194.96108 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pf1021bcace)\" d=\"M 240.778125 143.1 \nL 240.778125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.778125\" xlink:href=\"#mf7740f37c5\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(231.234375 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epochs -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(125.295312 171.376563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"304.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pf1021bcace)\" d=\"M 45.478125 124.671988 \nL 240.778125 124.671988 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m838ee61f63\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m838ee61f63\" y=\"124.671988\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 128.471207)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pf1021bcace)\" d=\"M 45.478125 59.024373 \nL 240.778125 59.024373 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m838ee61f63\" y=\"59.024373\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{3}}$ -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 62.823591)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"mdc8d1d34fd\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"139.23583\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"134.840932\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"131.033899\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"127.675858\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"104.910087\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"93.350116\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"85.148185\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"78.786274\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"73.588214\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"69.193317\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"65.386284\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"62.028243\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"39.262471\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"27.7025\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"19.50057\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"13.138659\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mdc8d1d34fd\" y=\"7.940599\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     </defs>\n     <g transform=\"translate(14.798437 84.807813)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#pf1021bcace)\" d=\"M 53.369034 136.922727 \nL 63.23267 136.752355 \nL 73.096307 125.487313 \nL 82.959943 129.209351 \nL 92.82358 93.986841 \nL 102.687216 119.756992 \nL 112.550852 73.348818 \nL 122.414489 86.837698 \nL 132.278125 77.718502 \nL 142.141761 68.454766 \nL 152.005398 67.79073 \nL 161.869034 49.428299 \nL 171.73267 52.791924 \nL 181.596307 59.475597 \nL 191.459943 49.3734 \nL 201.32358 35.220423 \nL 211.187216 44.016202 \nL 221.050852 23.183476 \nL 230.914489 13.377273 \nL 240.778125 15.252106 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path clip-path=\"url(#pf1021bcace)\" d=\"M 53.369034 130.040576 \nL 63.23267 127.554685 \nL 73.096307 129.176594 \nL 82.959943 128.58015 \nL 92.82358 125.724037 \nL 102.687216 127.621261 \nL 112.550852 118.73997 \nL 122.414489 121.492968 \nL 132.278125 120.114448 \nL 142.141761 116.100971 \nL 152.005398 119.458397 \nL 161.869034 103.842899 \nL 171.73267 106.814702 \nL 181.596307 110.458624 \nL 191.459943 100.090648 \nL 201.32358 89.61809 \nL 211.187216 90.675877 \nL 221.050852 83.748964 \nL 230.914489 75.390703 \nL 240.778125 80.885153 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 143.1 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 240.778125 143.1 \nL 240.778125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 143.1 \nL 240.778125 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 240.778125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 52.478125 44.55625 \nL 107.753125 44.55625 \nQ 109.753125 44.55625 109.753125 42.55625 \nL 109.753125 14.2 \nQ 109.753125 12.2 107.753125 12.2 \nL 52.478125 12.2 \nQ 50.478125 12.2 50.478125 14.2 \nL 50.478125 42.55625 \nQ 50.478125 44.55625 52.478125 44.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_34\">\n     <path d=\"M 54.478125 20.298438 \nL 74.478125 20.298438 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_35\"/>\n    <g id=\"text_10\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(82.478125 23.798438)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_36\">\n     <path d=\"M 54.478125 34.976563 \nL 74.478125 34.976563 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_37\"/>\n    <g id=\"text_11\">\n     <!-- test -->\n     <g transform=\"translate(82.478125 38.476563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf1021bcace\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0CLMOWxCk5g"
      },
      "source": [
        "\n",
        "### Using Weight Decay\n",
        "\n",
        "The example below shows that even though the training error increased,\n",
        "the error on the test set decreased.\n",
        "This is precisely the improvement that we expect from using weight decay.\n",
        "While not perfect, overfitting has been mitigated to some extent.\n",
        "In addition, the $\\ell_2$ norm of the weight $\\mathbf{w}$\n",
        "is smaller than without using weight decay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "id": "DUUBPFV-Ck5g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "f25d575b-d24f-4436-fe34-9d9e0fef955c"
      },
      "source": [
        "train(lambd=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l1 norm of w: 37.99309158325195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 257.521875 180.65625\" width=\"257.521875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 257.521875 180.65625 \nL 257.521875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 143.1 \nL 240.778125 143.1 \nL 240.778125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p056fa1089a)\" d=\"M 82.959943 143.1 \nL 82.959943 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m5ac96963b8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"82.959943\" xlink:href=\"#m5ac96963b8\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(76.597443 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p056fa1089a)\" d=\"M 122.414489 143.1 \nL 122.414489 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.414489\" xlink:href=\"#m5ac96963b8\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(116.051989 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p056fa1089a)\" d=\"M 161.869034 143.1 \nL 161.869034 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.869034\" xlink:href=\"#m5ac96963b8\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(155.506534 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p056fa1089a)\" d=\"M 201.32358 143.1 \nL 201.32358 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.32358\" xlink:href=\"#m5ac96963b8\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(194.96108 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p056fa1089a)\" d=\"M 240.778125 143.1 \nL 240.778125 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.778125\" xlink:href=\"#m5ac96963b8\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(231.234375 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epochs -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(125.295312 171.376563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"304.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p056fa1089a)\" d=\"M 45.478125 103.096076 \nL 240.778125 103.096076 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mecd7e9a826\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#mecd7e9a826\" y=\"103.096076\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 106.895294)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p056fa1089a)\" d=\"M 45.478125 55.067755 \nL 240.778125 55.067755 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#mecd7e9a826\" y=\"55.067755\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{3}}$ -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(20.878125 58.866974)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-49\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-48\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m8645a21906\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"136.666431\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"128.209064\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"122.208466\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"117.554041\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"113.751099\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"110.535757\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"107.750501\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"105.293731\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"88.63811\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"80.180743\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"74.180145\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"69.52572\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"65.722778\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"62.507436\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"59.72218\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"57.26541\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"40.60979\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"32.152422\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"26.151824\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"21.497399\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"17.694457\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"14.479115\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"11.693859\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m8645a21906\" y=\"9.237089\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     </defs>\n     <g transform=\"translate(14.798437 84.807812)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p056fa1089a)\" d=\"M 53.369034 114.710027 \nL 63.23267 100.380961 \nL 73.096307 98.190725 \nL 82.959943 86.276396 \nL 92.82358 83.361522 \nL 102.687216 83.502859 \nL 112.550852 59.375462 \nL 122.414489 72.103702 \nL 132.278125 43.327906 \nL 142.141761 63.4689 \nL 152.005398 46.462856 \nL 161.869034 43.533555 \nL 171.73267 52.436611 \nL 181.596307 23.751136 \nL 191.459943 46.87119 \nL 201.32358 22.76532 \nL 211.187216 27.773762 \nL 221.050852 28.938763 \nL 230.914489 13.377273 \nL 240.778125 14.284591 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p056fa1089a)\" d=\"M 53.369034 136.922727 \nL 63.23267 111.783017 \nL 73.096307 106.004573 \nL 82.959943 134.599278 \nL 92.82358 101.590151 \nL 102.687216 124.059203 \nL 112.550852 106.496992 \nL 122.414489 100.202097 \nL 132.278125 102.162092 \nL 142.141761 96.449053 \nL 152.005398 100.084665 \nL 161.869034 94.320549 \nL 171.73267 91.438054 \nL 181.596307 81.016401 \nL 191.459943 90.170096 \nL 201.32358 78.151999 \nL 211.187216 79.310484 \nL 221.050852 71.936578 \nL 230.914489 67.730516 \nL 240.778125 68.504691 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 143.1 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 240.778125 143.1 \nL 240.778125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 143.1 \nL 240.778125 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 240.778125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 52.478125 44.55625 \nL 107.753125 44.55625 \nQ 109.753125 44.55625 109.753125 42.55625 \nL 109.753125 14.2 \nQ 109.753125 12.2 107.753125 12.2 \nL 52.478125 12.2 \nQ 50.478125 12.2 50.478125 14.2 \nL 50.478125 42.55625 \nQ 50.478125 44.55625 52.478125 44.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_41\">\n     <path d=\"M 54.478125 20.298437 \nL 74.478125 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_42\"/>\n    <g id=\"text_10\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(82.478125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_43\">\n     <path d=\"M 54.478125 34.976562 \nL 74.478125 34.976562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_44\"/>\n    <g id=\"text_11\">\n     <!-- test -->\n     <g transform=\"translate(82.478125 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p056fa1089a\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2aOl7a3Ck5j"
      },
      "source": [
        "## Concise Implementation\n",
        "\n",
        "Because weight decay is ubiquitous in neural network optimization,\n",
        "Pytorch makes it especially convenient,\n",
        "integrating weight decay into the optimization algorithm itself\n",
        "for easy use in combination with any loss function.\n",
        "Moreover, this integration serves a computational benefit,\n",
        "allowing implementation tricks to add weight decay to the algorithm,\n",
        "without any additional computational overhead.\n",
        "Since the weight decay portion of the update\n",
        "depdends only on the current value of each parameter,\n",
        "and the optimizer must to touch each parameter once anyway.\n",
        "\n",
        "In the following code, we specify\n",
        "the weight decay hyper-parameter directly\n",
        "through the `weight_decay` parameter when instantiating our `Optimizer`.\n",
        "By default, Pytorch decays both weights and biases simultaneously.\n",
        "Per-Parameter or Parameters with different lr,weight_decay can be specified \n",
        "in the optimizer function using an iterable of `dict`s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr1UMpaECk5j"
      },
      "source": [
        "def fit_and_plot_pytorch(wd):\n",
        "    net = nn.Sequential(nn.Linear(200, 1))\n",
        "    #Random Uniform Initialization of the Model parameters\n",
        "    for param in net.parameters():\n",
        "        param.data.uniform_()\n",
        "    \n",
        "    loss = nn.MSELoss()\n",
        "    # The weight parameter has been decayed with its weight_decay set as wd\n",
        "    # The weight_decay for bias is by default zero (no decay)\n",
        "    optimizer = torch.optim.SGD([\n",
        "        {\"params\":net[0].weight,'weight_decay': wd},\n",
        "        {\"params\":net[0].bias}\n",
        "    ],lr=lr)\n",
        "    train_ls, test_ls = [], []\n",
        "    for _ in range(num_epochs):\n",
        "        for X, y in train_iter:\n",
        "            with torch.enable_grad():\n",
        "                optimizer.zero_grad()\n",
        "                l = loss(net(X), y)\n",
        "            l.backward()\n",
        "            # Call the step function to\n",
        "            # update the weight and bias\n",
        "            optimizer.step()\n",
        "        train_ls.append(torch.mean(loss(net(train_features),\n",
        "                             train_labels)).item())\n",
        "        test_ls.append(torch.mean(loss(net(test_features),\n",
        "                            test_labels)).item())\n",
        "    semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\n",
        "                 range(1, num_epochs + 1), test_ls, ['train', 'test'])\n",
        "    print('L2 norm of w:', net[0].weight.norm().item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gsa_JusCk5l"
      },
      "source": [
        "The plots look just the same as when we implemented weight decay from scratch\n",
        "but they run a bit faster and are easier to implement,\n",
        "a benefit that will become more pronounced for large problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRqH4IIOCk5m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b40eb2a2-cd38-4549-bdd7-0127f5ddfaf4"
      },
      "source": [
        "fit_and_plot_pytorch(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-af1b27569ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_and_plot_pytorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-140-d3793d38b956>\u001b[0m in \u001b[0;36mfit_and_plot_pytorch\u001b[0;34m(wd)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"params\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     ],lr=lr)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_ls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYsgVQziCk5o"
      },
      "source": [
        "fit_and_plot_pytorch(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hctmsj-vCk5q"
      },
      "source": [
        "So far, we only touched upon one notion of\n",
        "what constitutes a simple *linear* function.\n",
        "For nonlinear functions, what constitutes *simplicity*\n",
        "can be a far more complex question.\n",
        "For instance, there exist [Reproducing Kernel Hilbert Spaces (RKHS)](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)\n",
        "which allow one to use many of the tools\n",
        "introduced for linear functions in a nonlinear context.\n",
        "Unfortunately, RKHS-based algorithms\n",
        "do not always scale well to massive amounts of data.\n",
        "For the purposes of this book, we limit ourselves\n",
        "to simply summing over the weights for different layers,\n",
        "e.g. via $\\sum_l \\|\\mathbf{w}_l\\|^2$,\n",
        "which is equivalent to weight decay applied to all layers.\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "* Regularization is a common method for dealing with overfitting. It adds a penalty term to the loss function on the training set to reduce the complexity of the learned model.\n",
        "* One particular choice for keeping the model simple is weight decay using an $\\ell_2$ penalty. This leads to weight decay in the update steps of the learning algorithm.\n",
        "* Pytorch provides automatic weight decay functionality in the optimizer by setting the hyperparameter `weight_decay`.\n",
        "* You can have different lr,Weight_decay for different sets of parameters in an optimizer. They need to be passed as dicts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9JgqMRDCwWT"
      },
      "source": [
        "# Dropout\n",
        "\n",
        "Just now, we introduced the classical approach\n",
        "of regularizing statistical models by penalyzing\n",
        "the $\\ell_2$ norm of the weights.\n",
        "In probabilistic terms, we could justify this technique\n",
        "by arguing that we have assumed a prior belief\n",
        "that weights take values from a Gaussian distribution with mean $0$.\n",
        "More intuitively, we might argue\n",
        "that we encouraged the model to spread out its weights\n",
        "among many features and rather than depending too much\n",
        "on a small number of potentially spurious associations.\n",
        "\n",
        "## Overfitting Revisited\n",
        "\n",
        "Given many more features than examples, linear models can overfit.\n",
        "But when there are many more examples than features,\n",
        "we can generally count on linear models not to overfit.\n",
        "Unfortunately, the reliability with which linear models generalize\n",
        "comes at a cost:\n",
        "Linear models can’t take into account interactions among features.\n",
        "For every feature, a linear model must assign\n",
        "either a positive or a negative weight.\n",
        "They lack the flexibility to account for context.\n",
        "\n",
        "In more formal texts, you’ll see this fundamental tension\n",
        "between generalizability and flexibility\n",
        "discussed as the *bias-variance tradeoff*.\n",
        "Linear models have high bias\n",
        "(they can only represent a small class of functions),\n",
        "but low variance (they give similar results\n",
        "across different random samples of the data).\n",
        "\n",
        "Deep neural networks take us to the opposite end\n",
        "of the bias-variance spectrum.\n",
        "Neural networks are so flexible because\n",
        "they aren’t confined to looking at each feature individually.\n",
        "Instead, they can learn interactions among groups of features.\n",
        "For example, they might infer that “Nigeria” and “Western Union”\n",
        "appearing together in an email indicates spam\n",
        "but that “Nigeria” without “Western Union” does not.\n",
        "\n",
        "Even when we only have a small number of features,\n",
        "deep neural networks are capable of overfitting.\n",
        "In 2017, a group of researchers presented a now well-known\n",
        "demonstration of the incredible flexibility of neural networks.\n",
        "They presented a neural network with randomly-labeled images\n",
        "(there was no true pattern linking the inputs to the outputs)\n",
        "and found that the neural network, optimized by SGD,\n",
        "could label every image in the training set perfectly.\n",
        "\n",
        "Consider what this means.\n",
        "If the labels are assigned uniformly at random and there are 10 classes,\n",
        "then no classifier can get better than 10% accuracy on holdout data.\n",
        "Yet even in these situations, when there is no true pattern to be learned, neural networks can perfectly fit the training labels.\n",
        "\n",
        "## Robustness through Perturbations\n",
        "\n",
        "Let's think briefly about what we expect from a good statistical model.\n",
        "We want it to do well on unseen test data.\n",
        "One way we can accomplish this is by asking\n",
        "what constitutes a a 'simple' model?\n",
        "Simplicity can come in the form\n",
        "of a small number of dimensions,\n",
        "which is what we did when discussing fitting a model\n",
        "with monomial basis functions.\n",
        "Simplicity can also come in the form\n",
        "of a small norm for the basis functions.\n",
        "This led us to weight decay ($\\ell_2$ regularization).\n",
        "Yet a third notion of simplicity that we can impose\n",
        "is that the function should be robust\n",
        "under small changes in the input.\n",
        "For instance, when we classify images,\n",
        "we would expect that adding some random noise\n",
        "to the pixels should be mostly harmless.\n",
        "\n",
        "\n",
        "In 1995, Christopher Bishop formalized\n",
        "a form of this idea when he proved\n",
        "that [*training with input noise is equivalent to Tikhonov regularization*](https://www.mitpressjournals.org/doi/10.1162/neco.1995.7.1.108).\n",
        "In other words, he drew a clear mathematical connection\n",
        "between the requirement that a function be smooth (and thus simple),\n",
        "as we discussed in the section on weight decay,\n",
        "with and the requirement that it be resilient to perturbations in the input.\n",
        "\n",
        "Then in 2014, [Srivastava et al., 2014](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf),\n",
        "developed a clever idea for how to apply Bishop's idea\n",
        "to the *internal* layers of the network, too.\n",
        "Namely they proposed to inject noise into each layer of the network\n",
        "before calculating the subsequent layer during training.\n",
        "They realized that when training deep network with many layers,\n",
        "enforcing smoothness just on the input-output mapping\n",
        "misses out on what is happening internally in the network.\n",
        "Their proposed idea is called *dropout*,\n",
        "and it is now a standard technique\n",
        "that is widely used for training neural networks.\n",
        "Throughout training, on each iteration,\n",
        "dropout regularization consists simply of zeroing out\n",
        "some fraction (typically 50%) of the nodes in each layer\n",
        "before calculating the subsequent layer.\n",
        "\n",
        "The key challenge then is how to inject this noise\n",
        "without introducing undue statistical *bias*.\n",
        "In other words, we want to perturb the inputs\n",
        "to each layer during training\n",
        "in such a way that the expected value of the layer\n",
        "is equal to the value it would have taken\n",
        "had we not introduced any noise at all.\n",
        "\n",
        "In Bishop's case, when we are adding\n",
        "Gaussian noise to a linear model,\n",
        "this is simple:\n",
        "At each training iteration, just add noise\n",
        "sampled from a distribution with mean zero\n",
        "$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ to the input $\\mathbf{x}$ ,\n",
        "yielding a perturbed point $\\mathbf{x}' = \\mathbf{x} + \\epsilon$.\n",
        "In expectation, $\\mathbf{E}[\\mathbf{x}'] = \\mathbf{x}$.\n",
        "\n",
        "In the case of dropout regularization,\n",
        "one can debias each layer\n",
        "by normalizing by the fraction of nodes that were not dropped out.\n",
        "In other words, dropout with drop probability $p$ is applied as follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h' =\n",
        "\\begin{cases}\n",
        "    0 & \\text{ with probability } p \\\\\n",
        "    \\frac{h}{1-p} & \\text{ otherwise}\n",
        "\\end{cases}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "By design, the expectation remains unchanged,\n",
        "i.e., $\\mathbf{E}[h'] = h$.\n",
        "Intermediate activations $h$ are replaced by a random variable $h'$\n",
        "with matching expectation.\n",
        "The name 'dropout' arises from the notion\n",
        "that some neurons 'drop out' of the computation\n",
        "for the purpose of computing the final result.\n",
        "During training, we replace intermediate activations with random variables.\n",
        "\n",
        "## Dropout in Practice\n",
        "\n",
        "Recall the multilayer perceptron (:numref:`chapter_mlp`) with a hidden layer and 5 hidden units. Its architecture is given by\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    h & = \\sigma(W_1 x + b_1) \\\\\n",
        "    o & = W_2 h + b_2 \\\\\n",
        "    \\hat{y} & = \\mathrm{softmax}(o)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "When we apply dropout to the hidden layer,\n",
        "we are essentially removing each hidden unit with probability $p$,\n",
        "(i.e., setting their output to $0$).\n",
        "We can view the result as a network containing\n",
        "only a subset of the original neurons.\n",
        "In the image below, $h_2$ and $h_5$ are removed.\n",
        "Consequently, the calculation of $y$ no longer depends on $h_2$ and $h_5$\n",
        "and their respective gradient also vanishes when performing backprop.\n",
        "In this way, the calculation of the output layer\n",
        "cannot be overly dependent on any one element of $h_1, \\ldots, h_5$.\n",
        "Intuitively, deep learning researchers often explain the inutition thusly:\n",
        "we do not want the network's output to depend\n",
        "too precariously on the exact activation pathway through the network.\n",
        "The original authors of the dropout technique\n",
        "described their intuition as an effort\n",
        "to prevent the *co-adaptation* of feature detectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rqlpIrcCwWU"
      },
      "source": [
        "from IPython.display import SVG\n",
        "SVG(filename = '../img/dropout2.svg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SirMxKu4CwWY"
      },
      "source": [
        "At test time, we typically do not use dropout.\n",
        "However, we note that there are some exceptions:\n",
        "some researchers use dropout at test time as a heuristic appraoch\n",
        "for estimating the *confidence* of neural network predictions:\n",
        "if the predictions agree across many different dropout masks,\n",
        "then we might say that the network is more confident.\n",
        "For now we will put off the advanced topic of uncertainty estimation\n",
        "for subsequent chapters and volumes.\n",
        "\n",
        "\n",
        "## Implementation from Scratch\n",
        "\n",
        "To implement the dropout function for a single layer,\n",
        "we must draw as many samples from a Bernoulli (binary) random variable\n",
        "as our layer has dimensions, where the random variable takes value $1$ (keep) with probability $1-p$ and $0$ (drop) with probability $p$.\n",
        "One easy way to implement this is to first draw samples\n",
        "from the uniform distribution $U[0,1]$.\n",
        "then we can keep those nodes for which the corresponding\n",
        "sample is greater than $p$, dropping the rest.\n",
        "\n",
        "In the following code, we implement a `dropout` function\n",
        "that drops out the elements in the Tensor input `X`\n",
        "with probability `drop_prob`,\n",
        "rescaling the remainder as described above\n",
        "(dividing the survivors by `1.0-drop_prob`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTVRmSJQCwWY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils import data\n",
        "%config InlineBackend.figure_formats = ['svg']\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from IPython import display\n",
        "sys.path.insert(0, '..')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz17Tub7CwWb"
      },
      "source": [
        "def dropout(X, drop_prob):\n",
        "    assert 0 <= drop_prob <= 1\n",
        "    # In this case, all elements are dropped out\n",
        "    if drop_prob == 1:\n",
        "        return torch.zeros_like(X)\n",
        "    mask = (torch.FloatTensor(X.shape).uniform_(0, 1) > drop_prob).float()\n",
        "    return mask * X / (1.0 - drop_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9u9bdcDCwWd"
      },
      "source": [
        "We can test out the `dropout` function on a few examples.\n",
        "In the following lines of code, we pass our input `X`\n",
        "through the dropout operation, with probabilities 0, 0.5, and 1, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNBYYsckCwWd"
      },
      "source": [
        "X = torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
        "print(X)\n",
        "print(dropout(X, 0.))\n",
        "print(dropout(X, 0.5))\n",
        "print(dropout(X, 1.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fiIRIn0CwWg"
      },
      "source": [
        "### Defining Model Parameters\n",
        "\n",
        "Again, we can use the Fashion-MNIST dataset,\n",
        "introduced in :numref:`chapter_softmax_scratch`.\n",
        "We will define a multilayer perceptron with two hidden layers.\n",
        "The two hidden layers both have 256 outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2GYQOTQCwWg"
      },
      "source": [
        "### Define the Model\n",
        "\n",
        "The model defined below concatenates the fully-connected layer\n",
        " and the activation function ReLU,\n",
        " using dropout for the output of each activation function.\n",
        " We can set the dropout probability of each layer separately.\n",
        " It is generally recommended to set\n",
        " a lower dropout probability closer to the input layer.\n",
        " Below we set it to 0.2 and 0.5 for the first and second hidden layer respectively.\n",
        " By using the `is_training` function described in :numref:`chapter_autograd`,\n",
        " we can ensure that dropout is only active during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyb9cbxUCwWh"
      },
      "source": [
        "drop_prob1, drop_prob2 = 0.2, 0.5\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_inputs = 784, num_outputs = 10, num_hiddens1 = 256, num_hiddens2 = 256, is_training = True):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.num_hiddens1 = num_hiddens1\n",
        "        self.num_hiddens2 = num_hiddens2\n",
        "        self.is_training = is_training\n",
        "        \n",
        "        self.linear_1 = nn.Linear(num_inputs, num_hiddens1)\n",
        "        self.linear_2 = nn.Linear(num_hiddens1, num_hiddens2)\n",
        "        self.linear_3 = nn.Linear(num_hiddens2, num_outputs)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, X):\n",
        "        X = X.reshape((-1, self.num_inputs))\n",
        "        H1 = self.relu(self.linear_1(X))\n",
        "        # Use dropout only when training the model\n",
        "        if self.is_training == True:\n",
        "            # Add a dropout layer after the first fully connected layer\n",
        "            H1 = dropout(H1, drop_prob1)\n",
        "        H2 = self.relu(self.linear_2(H1))\n",
        "        if self.is_training == True:\n",
        "            # Add a dropout layer after the second fully connected layer\n",
        "            H2 = dropout(H2, drop_prob2)\n",
        "        out = self.linear_3(H2)\n",
        "        return out\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbT0vaLyCwWj"
      },
      "source": [
        "### Training and Testing\n",
        "\n",
        "This is similar to the training and testing of multilayer perceptrons described previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuxekzWOL98G"
      },
      "source": [
        "def get_dataloader_workers(num_workers=4):  \n",
        "    # 0 means no additional process is used to speed up the reading of data.\n",
        "    if sys.platform.startswith('win'):\n",
        "        return 0\n",
        "    else:\n",
        "        return num_workers\n",
        "\n",
        "def load_data_fashion_mnist(batch_size, resize=None):  \n",
        "    \"\"\"Download the Fashion-MNIST dataset and then load into memory.\"\"\"\n",
        "    trans = [transforms.Resize(resize)] if resize else []\n",
        "    trans.append(transforms.ToTensor())\n",
        "    trans = transforms.Compose(trans)\n",
        "\n",
        "    mnist_train = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=True, transform=trans, download=True)\n",
        "    mnist_test = torchvision.datasets.FashionMNIST(\n",
        "        root=\"../data\", train=False, transform=trans, download=True)\n",
        "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
        "                            num_workers=get_dataloader_workers()),\n",
        "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
        "                            num_workers=get_dataloader_workers()))\n",
        "\n",
        "\n",
        "\n",
        "def train_epoch_ch3(net, train_iter, loss, updater):  \n",
        "    metric = Accumulator(3)  # train_loss_sum, train_acc_sum, num_examples\n",
        "    pt_optimizer=False\n",
        "    if isinstance(updater, torch.optim.Optimizer):\n",
        "        pt_optimizer=True\n",
        "    for X, y in train_iter:\n",
        "        # Compute gradients and update parameters\n",
        "        y_hat = net(X)\n",
        "        l = loss(y_hat, y)\n",
        "        if pt_optimizer:\n",
        "            updater.zero_grad()\n",
        "            l.backward()\n",
        "            updater.step()\n",
        "            metric.add(float(l)*len(y), float(accuracy(y_hat, y)), len(y))\n",
        "        else:\n",
        "            l.sum().backward()\n",
        "            updater(X.shape[0])\n",
        "            l_sum = float(l.sum())\n",
        "            metric.add(l_sum, accuracy(y_hat, y), y.numpy().size)\n",
        "    # Return training loss and training accuracy\n",
        "    return metric[0]/metric[2], metric[1]/metric[2]\n",
        "\n",
        "def accuracy(y_hat, y):  \n",
        "    if y_hat.shape[1] > 1:\n",
        "        return float((y_hat.argmax(axis=1).type(torch.float32) ==\n",
        "                      y.type(torch.float32)).sum())\n",
        "    else:\n",
        "        return float((y_hat.type(torch.int32) == y.type(torch.int32)).sum())\n",
        "\n",
        "def evaluate_accuracy(net, data_iter):  \n",
        "    metric = Accumulator(2)  # num_corrected_examples, num_examples\n",
        "    for X, y in data_iter:\n",
        "        metric.add(accuracy(net(X), y), y.numpy().size)\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
        "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                        ylim=[0.3, 0.9],\n",
        "                        legend=['train loss', 'train acc', 'test acc'])\n",
        "    for epoch in range(num_epochs):\n",
        "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
        "        test_acc = evaluate_accuracy(net, test_iter)\n",
        "        animator.add(epoch+1, train_metrics+(test_acc,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na9n5IT9CwWj"
      },
      "source": [
        "num_epochs, lr, batch_size = 10, 0.5, 256\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.SGD(net.parameters(), lr) \n",
        "train_ch3(net, train_iter, test_iter, criterion, num_epochs, trainer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGi-f0UTCwWm"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Beyond controlling the number of dimensions and the size of the weight vector, dropout is yet another tool to avoid overfitting. Often all three are used jointly.\n",
        "* Dropout replaces an activation $h$ with a random variable $h'$ with expected value $h$ and with variance given by the dropout probability $p$.\n",
        "* Dropout is only used during training.\n",
        "\n",
        "## References\n",
        "\n",
        "[1] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).  JMLR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64lUkR5vC-F6"
      },
      "source": [
        "# Forward Propagation, Backward Propagation, and Computational Graphs\n",
        "\n",
        "In the previous sections, we used mini-batch\n",
        "stochastic gradient descent to train our models.\n",
        "When we implemented the algorithm,\n",
        "we only worried about the calculations involved\n",
        "in *forward propagation* through the model.\n",
        "In other words, we implemented the calculations\n",
        "required for the model to generate output\n",
        "corresponding to some given input,\n",
        "but when it came time to calculate the gradients of each of our parameters,\n",
        "we invoked the `backward` function,\n",
        "relying on the `torch.autograd` module to figure out what to do.\n",
        "\n",
        "The automatic calculation of gradients profoundly simplifies\n",
        "the implementation of deep learning algorithms.\n",
        "Before automatic differentiation,\n",
        "even small changes to complicated models would require\n",
        "recalculating lots of derivatives by hand.\n",
        "Even academic papers would too often have to allocate\n",
        "lots of page real estate to deriving update rules.\n",
        "\n",
        "While we plan to continue relying on `torch.autograd`,\n",
        "and we have already come a long way\n",
        "without even discussing how these gradients\n",
        "are calculated efficiently under the hood,\n",
        "it's important that you know\n",
        "how updates are actually calculated\n",
        "if you want to go beyond a shallow understanding of deep learning.\n",
        "\n",
        "In this section, we'll peel back the curtain on some of the details\n",
        "of backward propagation (more commonly called *backpropagation* or *backprop*).\n",
        "To convey some insight for both the techniques and how they are implemented,\n",
        "we will rely on both mathematics and computational graphs\n",
        "to describe the mechanics behind neural network computations.\n",
        "To start, we will focus our exposition on\n",
        "a simple multilayer perceptron with a single hidden layer\n",
        "and $\\ell_2$ norm regularization.\n",
        "\n",
        "\n",
        "## Forward Propagation\n",
        "\n",
        "Forward propagation refers to the calculation and storage\n",
        "of intermediate variables (including outputs)\n",
        "for the neural network within the models\n",
        "in the order from input layer to output layer.\n",
        "In the following, we work in detail through the example of a deep network\n",
        "with one hidden layer step by step.\n",
        "This is a bit tedious but it will serve us well\n",
        "when discussing what really goes on when we call `backward`.\n",
        "\n",
        "For the sake of simplicity, let’s assume\n",
        "that the input example is $\\mathbf{x}\\in \\mathbb{R}^d$\n",
        "and there is no bias term.\n",
        "Here the intermediate variable is:\n",
        "\n",
        "$$\\mathbf{z}= \\mathbf{W}^{(1)} \\mathbf{x}$$\n",
        "\n",
        "$\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$\n",
        "is the weight parameter of the hidden layer.\n",
        "After entering the intermediate variable $\\mathbf{z}\\in \\mathbb{R}^h$\n",
        "into the activation function $\\phi$ operated by the basic elements,\n",
        "we will obtain a hidden layer variable with the vector length of $h$,\n",
        "\n",
        "$$\\mathbf{h}= \\phi (\\mathbf{z}).$$\n",
        "\n",
        "The hidden variable $\\mathbf{h}$ is also an intermediate variable.\n",
        "Assuming the parameters of the output layer\n",
        "only possess a weight of $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$,\n",
        "we can obtain an output layer variable with a vector length of $q$:\n",
        "\n",
        "$$\\mathbf{o}= \\mathbf{W}^{(2)} \\mathbf{h}.$$\n",
        "\n",
        "Assuming the loss function is $l$ and the example label is $y$,\n",
        "we can then calculate the loss term for a single data example,\n",
        "\n",
        "$$L = l(\\mathbf{o}, y).$$\n",
        "\n",
        "According to the definition of $\\ell_2$ norm regularization,\n",
        "given the hyper-parameter $\\lambda$, the regularization term is\n",
        "\n",
        "$$s = \\frac{\\lambda}{2} \\left(\\|\\mathbf{W}^{(1)}\\|_F^2 + \\|\\mathbf{W}^{(2)}\\|_F^2\\right),$$\n",
        "\n",
        "where the Frobenius norm of the matrix is equivalent\n",
        "to the calculation of the $L_2$ norm\n",
        "after flattening the matrix to a vector.\n",
        "Finally, the model's regularized loss on a given data example is\n",
        "\n",
        "$$J = L + s.$$\n",
        "\n",
        "We refer to $J$ as the objective function of a given data example\n",
        "and refer to it as the ‘objective function’ in the following discussion.\n",
        "\n",
        "\n",
        "## Computational Graph of Forward Propagation\n",
        "\n",
        "Plotting computational graphs helps us visualize\n",
        "the dependencies of operators and variables within the calculation.\n",
        "The figure below contains the graph associated\n",
        "with the simple network described above.\n",
        "The lower-left corner signifies the input\n",
        "and the upper right corner the output.\n",
        "Notice that the direction of the arrows (which illustrate data flow)\n",
        "are primarily rightward and upward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YShnXAU9C-F6"
      },
      "source": [
        "from IPython.display import SVG\n",
        "SVG(filename=\"../img/forward.svg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXrJCAtvC-F-"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "Backpropagation refers to the method of calculating\n",
        "the gradient of neural network parameters.\n",
        "In general, back propagation calculates and stores\n",
        "the intermediate variables of an objective function\n",
        "related to each layer of the neural network\n",
        "and the gradient of the parameters\n",
        "in the order of the output layer to the input layer\n",
        "according to the ‘chain rule’ in calculus.\n",
        "Assume that we have functions $\\mathsf{Y}=f(\\mathsf{X})$\n",
        "and $\\mathsf{Z}=g(\\mathsf{Y}) = g \\circ f(\\mathsf{X})$,\n",
        "in which the input and the output\n",
        "$\\mathsf{X}, \\mathsf{Y}, \\mathsf{Z}$\n",
        "are tensors of arbitrary shapes.\n",
        "By using the chain rule, we can compute\n",
        "the derivative of $\\mathsf{Z}$ wrt. $\\mathsf{X}$ via\n",
        "\n",
        "$$\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{X}} = \\text{prod}\\left(\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{Y}}, \\frac{\\partial \\mathsf{Y}}{\\partial \\mathsf{X}}\\right).$$\n",
        "\n",
        "Here we use the $\\text{prod}$ operator\n",
        "to multiply its arguments after the necessary operations,\n",
        "such as transposition and swapping input positions have been carried out.\n",
        "For vectors, this is straightforward:\n",
        "it is simply matrix-matrix multiplication\n",
        "and for higher dimensional tensors we use the appropriate counterpart.\n",
        "The operator $\\text{prod}$ hides all the notation overhead.\n",
        "\n",
        "The parameters of the simple network with one hidden layer\n",
        "are $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$.\n",
        "The objective of backpropagation is to\n",
        "calculate the gradients $\\partial J/\\partial \\mathbf{W}^{(1)}$\n",
        "and $\\partial J/\\partial \\mathbf{W}^{(2)}$.\n",
        "To accomplish this, we will apply the chain rule\n",
        "and calculate, in turn, the gradient of\n",
        "each intermediate variable and parameter.\n",
        "The order of calculations are reversed\n",
        "relative to those performed in forward propagation,\n",
        "since we need to start with the outcome of the compute graph\n",
        "and work our way towards the parameters.\n",
        "The first step is to calculate the gradients\n",
        "of the objective function $J=L+s$\n",
        "with respect to the loss term $L$\n",
        "and the regularization term $s$.\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial L} = 1 \\text{ and } \\frac{\\partial J}{\\partial s} = 1$$\n",
        "\n",
        "Next, we compute the gradient of the objective function\n",
        "with respect to variable of the output layer $\\mathbf{o}$\n",
        "according to the chain rule.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{o}}\n",
        "= \\text{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\mathbf{o}}\\right)\n",
        "= \\frac{\\partial L}{\\partial \\mathbf{o}}\n",
        "\\in \\mathbb{R}^q\n",
        "$$\n",
        "\n",
        "Next, we calculate the gradients of the regularization term\n",
        "with respect to both parameters.\n",
        "\n",
        "$$\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)}\n",
        "\\text{ and }\n",
        "\\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}$$\n",
        "\n",
        "Now we are able calculate the gradient\n",
        "$\\partial J/\\partial \\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$\n",
        "of the model parameters closest to the output layer.\n",
        "Using the chain rule yields:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}\n",
        "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}\\right)\n",
        "= \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}\n",
        "$$\n",
        "\n",
        "To obtain the gradient with respect to $\\mathbf{W}^{(1)}$\n",
        "we need to continue backpropagation\n",
        "along the output layer to the hidden layer.\n",
        "The gradient with respect to the hidden layer's outputs\n",
        "$\\partial J/\\partial \\mathbf{h} \\in \\mathbb{R}^h$ is given by\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{h}}\n",
        "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}}\\right)\n",
        "= {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}.\n",
        "$$\n",
        "\n",
        "Since the activation function $\\phi$ applies element-wise,\n",
        "calculating the gradient $\\partial J/\\partial \\mathbf{z} \\in \\mathbb{R}^h$\n",
        "of the intermediate variable $\\mathbf{z}$\n",
        "requires that we use the element-wise multiplication operator,\n",
        "which we denote by $\\odot$.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{z}}\n",
        "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{h}}, \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\\right)\n",
        "= \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'\\left(\\mathbf{z}\\right).\n",
        "$$\n",
        "\n",
        "Finally, we can obtain the gradient\n",
        "$\\partial J/\\partial \\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$\n",
        "of the model parameters closest to the input layer.\n",
        "According to the chain rule, we get\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
        "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{z}}, \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}\\right)\n",
        "= \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}.\n",
        "$$\n",
        "\n",
        "## Training a Model\n",
        "\n",
        "When training networks, forward and backward propagation depend on each other. In particular, for forward propagation,\n",
        "we traverse the compute graph in the direction of dependencies\n",
        "and compute all the variables on its path.\n",
        "These are then used for backpropagation\n",
        "where the compute order on the graph is reversed.\n",
        "One of the consequences is that we need to retain\n",
        "the intermediate values until backpropagation is complete.\n",
        "This is also one of the reasons why backpropagation\n",
        "requires significantly more memory than plain 'inference'—we end up\n",
        "computing tensors as gradients\n",
        "and need to retain all the intermediate variables\n",
        "to invoke the chain rule.\n",
        "Another reason is that we typically train\n",
        "with minibatches containing more than one variable,\n",
        "thus more intermediate activations need to be stored.\n",
        "\n",
        "## Summary\n",
        "\n",
        "* Forward propagation sequentially calculates and stores intermediate variables within the compute graph defined by the neural network. It proceeds from input to output layer.\n",
        "* Back propagation sequentially calculates and stores the gradients of intermediate variables and parameters within the neural network in the reversed order.\n",
        "* When training deep learning models, forward propagation and back propagation are interdependent.\n",
        "* Training requires significantly more memory and storage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWxxP4HTJk9B"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6jd5gGCQHfD"
      },
      "source": [
        "\n",
        "\n",
        "# Activation Functions\n",
        "\n",
        "Because they are so fundamental to deep learning, before going further,\n",
        "let's take a brief look at some common activation functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufuycwllJRUx"
      },
      "source": [
        "\n",
        "## ReLU Function\n",
        "\n",
        "As stated above, the most popular choice,\n",
        "due to its simplicity of implementation\n",
        "and its efficacy in training is the rectified linear unit (ReLU).\n",
        "ReLUs provide a very simple nonlinear transformation.\n",
        "Given the element $z$, the function is defined\n",
        "as the maximum of that element and 0.\n",
        "\n",
        "$$\\mathrm{ReLU}(z) = \\max(z, 0).$$\n",
        "\n",
        "It can be understood that the ReLU function retains only positive elements and discards negative elements (setting those nodes to 0).\n",
        "To get a better idea of what it looks like, we can plot it.\n",
        "For convenience, we define a plotting function `xyplot`\n",
        "to take care of the groundwork."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUOI-LhGQHfE"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def xyplot(x_vals,y_vals,name):\n",
        "    x_vals = x_vals.detach().numpy() # we can't directly use var.numpy() because varibles might \n",
        "    y_vals = y_vals.detach().numpy() # already required grad.,thus using var.detach().numpy() \n",
        "    plt.plot(x_vals,y_vals) \n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel(name+'(x)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSWo97pJQHfG"
      },
      "source": [
        "Since relu is commomly used as activation function, PyTorch supports\n",
        "the `relu` function as a basic native operator.\n",
        "As you can see, the activation function is piece-wise linear.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvKuNcq1QHfG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "28122d74-25ef-4a89-b3c7-02e493b15c64"
      },
      "source": [
        "x = Variable(torch.arange(-8.0,8.0,0.1,dtype=torch.float32).reshape(int(16/0.1),1), requires_grad=True)\n",
        "y = torch.nn.functional.relu(x)\n",
        "xyplot(x,y,'relu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAd7ElEQVR4nO3deXiU5dn+8e8li8gmKAGUXREQkSVEFLcWxRb3ttYFoYu2YtlEXqtVW+vPrlatS13oS6vdCCAial2r1t0qmo193zchiOyEhOT6/TETG3kBk0meeWaeOT/HwdEsk7nPQrxy556Z5zR3R0REouewsAOIiEgwNOBFRCJKA15EJKI04EVEIkoDXkQkouqHHaCqVq1aeefOncOOISKSNvLz8ze7e9aBPpdSA75z587k5eWFHUNEJG2Y2aqDfU5HNCIiEaUBLyISURrwIiIRFeiAN7PxZjbPzOaa2RQzaxTkeiIi8l+BDXgzawfcAOS4ey+gHnBVUOuJiMgXBX1EUx84wszqA42B9QGvJyIicYENeHdfB9wHrAY2ANvc/dX9b2dmI8wsz8zyiouLg4ojIpJxgjyiaQlcCnQBjgWamNnw/W/n7hPdPcfdc7KyDvhcfRGRyJq5/FMef28FQVy6PcgjmsHACncvdvcyYAZweoDriYiklU07ShgzpZBJH65iT1l5nd9/kAN+NXCamTU2MwPOBRYEuJ6ISNrYV17BuClF7CgpY8LwbBo3rPsLCwR5Bj8TmA4UAHPia00Maj0RkXTywOuL+WD5p/zy0l70aNs8kDUCvRaNu98J3BnkGiIi6eaNhRt59M1lXJnTgctzOgS2jl7JKiKSRGs/2834J2dx4jHNuevSkwJdSwNeRCRJ9u4rZ3RuARUVzoRh2TRqUC/Q9VLqcsEiIlH26xcXMGvtNv44vD+dWzUJfD3t4EVEkuC5onX8/YNVXHdWF4b0apuUNTXgRUQCtnTTDm6bMYecTi25ZUiPpK2rAS8iEqDdpfsYOamAIxrU45Grs2lQL3ljV2fwIiIBcXdunzGHpcU7+ce1p9L2yOReMV07eBGRgEz+aDXPFq1n/OBunHlCq6SvrwEvIhKAOWu3cdc/53N2tyzGDOoaSgYNeBGROrZtdxkjc/Np1bQhD17Zl8MOs1By6AxeRKQOVVQ4Nz1VxMbtJTx5/UCOatIwtCzawYuI1KGJ7y7n9QWbuP2CE8nu2DLULBrwIiJ15MPln3LvvxZx4cnH8P3TO4cdRwNeRKQubNpRwtgphXQ6qjF3X3YysRqMcOkMXkSklvaVV3DDlEJ2lJTxjx8MoFmjBmFHAjTgRURq7f7XFvPh8i38/vI+gZV3JCLI0u3uZlZU5c92M7sxqPVERMLw7wUbeeytZQwd0IHL+rcPO84XBLaDd/dFQF8AM6sHrAOeCWo9EZFkW7NlN/8zbRYnHducOy8OtrwjEcl6kPVcYJm7r0rSeiIigdq7r5zRkwuocOexJJR3JCJZA/4qYMqBPmFmI8wsz8zyiouLkxRHRKR2fvXCAmav3cZ9l/eh09HBl3ckIvABb2YNgUuApw70eXef6O457p6TlZUVdBwRkVp7rmgd//hwFSPOPo6vn5Sc8o5EJGMHfz5Q4O4bk7CWiEiglmyMlXec0rklN3+9e9hxDikZA34oBzmeERFJJ7v27mNkbgGNGya/vCMRgaYzsybAecCMINcREQmau3P7M3NYXryTh67qR5vmyS3vSESgL3Ry913A0UGuISKSDLkzV/Nc0XpuOq8bZ3RNfnlHIlL79wsRkRQwe+1WfvH8fL7aPYvRIZV3JEIDXkTkELbuLmXkpAJaNW3IA1eEV96RCF2LRkTkICoqnJumzWLTjhKmXT+QliGWdyRCO3gRkYP44zvL+PfCTfzswp70C7m8IxEa8CIiB/DBsk+571+LuKj3MXx3YKew4yREA15EZD+btsfKOzq3asLdl/VOifKOROgMXkSkin3lFYydUsjOvWXk/vBUmh6evmMyfZOLiATg968tZuaKLdx/RR+6t20Wdpxa0RGNiEjc6/M3MuGtZQwd0JFvZadWeUciNOBFRKgs7yiKl3f0DDtOndCAF5GMV1JWzsjcfByYMKx/SpZ3JEJn8CKS8X75wnzmrtvOn76bQ8ejG4cdp85oBy8iGe3ZwnXkzlzN9Wcfx3k924Qdp05pwItIxqos7xjQ+Sh+nOLlHYnQgBeRjLRr7z5+NCmfJofX4+Gr+6V8eUcidAYvIhnH3bl1xhxWbN7FpB+emhblHYkIutGphZlNN7OFZrbAzAYGuZ6ISHVM+nAVz89az01f687px6dHeUcigt7BPwS84u7fNrOGQHQenhaRtDRrzVZ+8cJ8BnXPYuRXjg87TqACG/BmdiRwNvB9AHcvBUqDWk9E5Mts3V3KqNwCWjdrxANXpld5RyKCPKLpAhQDfzGzQjP7c7yE+wvMbISZ5ZlZXnFxcYBxRCSTVVQ4/xMv73h0WDYtGqdXeUcighzw9YFsYIK79wN2AbfufyN3n+juOe6ek5WVFWAcEclkE95exhsLN3HHRT3p26FF2HGSIsgBvxZY6+4z4+9PJzbwRUSS6j/LNvP7VxdxcZ9j+c5p6VnekYjABry7fwKsMbPKVw+cC8wPaj0RkQPZuL2EG6YU0qVVE377rZPTtrwjEUE/i2YskBt/Bs1y4JqA1xMR+VxleceuveVMvu60tC7vSESg/2/dvQjICXINEZGDue/VxXy0YgsPXtmXbm3Su7wjEdF7ba6ICPDa/I388e1lDDu1I9/o1y7sOKHQgBeRyFn96W5umlZEr3bNueOiaJR3JEIDXkQipaSsnFGT84FolXckIrMecRCRyPtFlfKODkdl9tVRtIMXkch4pnAtk2eu5kdfOT5y5R2J0IAXkUhYvHEHt8+Yy4AuR/Hjr3ULO05K0IAXkbS38/Pyjvo8MrQf9SNY3pEI/S2ISFpzd26bMYeVm3fx8NB+tI5oeUciNOBFJK39o0p5x8Djjw47TkrRgBeRtFW0Ziu/fGE+5/RoHfnyjkRowItIWvpsVymj4+Ud91/RJ/LlHYnQ8+BFJO1UVDjjpxVRvGMv00cOzIjyjkRoBy8iaWfC28t4a1Exd1zck97tM6O8IxEa8CKSVv6zNFbecWnfYxl+asew46Q0DXgRSRsbt5dww9RCjstqym++mVnlHYnQGbyIpIWy8grGTC5g195yplyXTZMMK+9IRKB/Q2a2EtgBlAP73F3lHyKSkPv+tYiPV37GQ1f15YQMLO9IRDJ+BA5y981JWEdEIurVeZ/wv+8sZ/hpHbm0b2aWdyRCZ/AiktJWf7qbm56aRe/2R2Z0eUcigh7wDrxqZvlmNuJANzCzEWaWZ2Z5xcXFAccRkXRSUlbOyNx8DjPj0auzObx+5pZ3JCLoAX+mu2cD5wOjzezs/W/g7hPdPcfdc7KysgKOIyLp5K7n5zNv/Xbuv6JPxpd3JCLQAe/u6+L/uwl4BhgQ5HoiEh1P569lykerGfnV4zn3RJV3JCKwAW9mTcysWeXbwNeAuUGtJyLRsfCT7fz02Tmc2uUobjpP5R2JCvJZNG2AZ+IvRKgPTHb3VwJcT0QiYOfefYzKLaBZowY8fLXKO2ojsAHv7suBPkHdv4hEj7vzk6dns3LzLiZfdxqtm6m8ozb0o1FEUsbfP1jFi7M3cPPXe3DacSrvqC0NeBFJCYWrP+NXL85n8Imtuf7s48KOEwka8CISusryjjbNG/H7y/uqvKOO6Go9IhKqigrnxieL2LyzlOkjB3Jk4wZhR4oM7eBFJFSPvrmUtxcX83OVd9Q5DXgRCc37Szdz/+uL+UbfYxmm8o46V+MBH38Bky4IISK18sm2EsZNLaRrVlN+rfKOQHzpgDezw8zsajN70cw2AQuBDWY238zuNbOuwccUkSgpK69g7JQCdpeWM2G4yjuCUp0d/JvA8cBtQFt37+DurYEzgQ+B35nZ8AAzikjE3Bsv77j7st50ba3yjqBU58fmYHcv2/+D7r4FeBp42sz0sLeIVMsrcz9h4jvL+c5pnbikz7Fhx4m0L93BVw53Mxu8/+fM7HtVbyMiciirPt3FzfHyjp9ddGLYcSKvJg+y/tzMJsQfZG1jZs8DFwcVTESipaSsnJGTCjjsMJV3JEtNBvxXgGVAEfAesatDfjuQVCISOXc9P4/5G7bzwJUq70iWmgz4lsQKO5YBe4FOpuc1iUg1TM9fy5SP1jB60PGc00PlHclSkwH/IfCKuw8BTgGOBd4PJJWIRMbCT7bzs2fnMPC4oxk/WOUdyVSTJ58OdvfVAO6+B7jhQB2rIiKVdpSUMXJSrLzjoaF9Vd6RZNV5oVNngMrhXpW7v2Mx7Q/x9fXMrNDMXqhNUBFJL+7OrU/PYfWW3TwytJ/KO0JQnR38vWZ2GPAckA8UA42ArsAg4FzgTmDtQb5+HLAAaF7rtCKSNv76n5W8OGcDt57fg1NV3hGKLx3w7n65mfUEhgHXAscAe4gN7ReBX7t7yYG+Nr6zvxD4NfA/dRVaRFJbwerP+M1LCxh8YhtGnKXyjrBU6wze3ecDP03g/h8EbgEO+lpkMxsBjADo2FFXkxNJd1t2lTImt4C2Rzbi95f3UXlHiKr9IKuZffdAH3f3vx/k9hcBm9w938y+erD7dfeJwESAnJwcr24eEUk9n5d37CplxsjTVd4Rspo8i+aUKm83Inb2XgAccMADZwCXmNkF8ds3N7NJ7q4Lk4lE1CNvLuWdxcX85psn06vdkWHHyXjVHvDuPrbq+2bWAph6iNvfRuwKlMR38D/WcBeJrveWbOaB1xfzzX7tGDqgQ9hxhNo1Ou0CutRVEBFJXxu27eGGqYWc0Lopv/5mL5V3pIianME/D1SekR8G9ASmVedr3f0t4K0aZhORNFBWXsGYyYWUlJXz2LD+NG6o8o5UUZN/ifuqvL0PWOXuB3vuu4hkiHteWUj+qs/4w9B+dG3dNOw4UkVNzuDfDjKIiKSfV+Zu4E/vruB7A1XekYq+dMCb2Q7+ezTzhU8B7u56hapIBlq5eRc3PzWbPh1acPuFKu9IRdV5JasKE0XkC0rKyhmZW1ne0U/lHSmqRs+iMbMzzeya+NutzEzPohHJQHc+N48FG7bz4JV9ad9S5R2pqtoD3szuBH5C/LntQENgUhChRCR1PZW3hifz1jBmUFcG9Wgddhw5hJrs4L8JXELs+e+4+3oOcY0ZEYmeBRu2c8dzczn9+KMZf57KO1JdTQZ8qbs78QdczaxJMJFEJBXtKCljVG4BzRs14KGr+lFPFxFLedUa8PHu1RfM7H+BFmZ2HfA68Kcgw4lIanB3fvL07Fh5x9XZZDU7POxIUg3VvVywm9nlxK7pvh3oDvzc3V8LMpyIpIa/vL+Sl+Z8wu0X9GBAl6PCjiPVVJNXshYAW9395qDCiEjqyV8VK+84r2cbrlN5R1qpyYA/FRhmZquIP9AK4O696zyViKSELbtKGTO5gGNaNOK+y/voImJppiYD/uuBpRCRlFNe4YybWsinleUdR6i8I93U5Fo0q4IMIiKp5ZE3lvLuks389lsq70hXtbkevIhE1LtLinnw34v5VnY7rjpF5R3pSgNeRL5gw7Y9jJtaRLfWzfjVN1Tekc4CG/Bm1sjMPjKzWWY2z8zuCmotEakbZeUVjM4tYG9ZOY8Nz1Z5R5oL8l9vL3COu+80swbAe2b2srt/GOCaIlILd7+8kILVW3l4aD+Oz1J5R7oLbMDHL2uwM/5ug/ifA11XXkRSwMtzNvD4eyv4/umduVjlHZEQ6Bm8mdUzsyJgE/Cau888wG1GmFmemeUVFxcHGUdEDmLF5l3cMn02fTu04PYLVN4RFYEOeHcvd/e+QHtggJn1OsBtJrp7jrvnZGVlBRlHRA6gpKyckZPyqVfPeHRYNg3r67kXUZGUf0l33wq8CQxJxnoiUn0/f24uizbu4MEr+9KuxRFhx5E6FOSzaLLMrEX87SOA84CFQa0nIjU37eM1TMtby9hBXflqd5V3RE2Qz6I5BvibmdUj9oNkmru/EOB6IlID89fHyjvO6Ho04warvCOKgnwWzWygX1D3LyKJ215SxqjcfFo0VnlHlOlVDCIZxt35yfTZrPlsD1NHnEarpirviCo9XC6SYZ54fyUvz/2EW4f04JTOKu+IMg14kQySv2oLv31pAV/r2YYfntUl7DgSMA14kQzx6c69jM4tpF3LI7hX5R0ZQWfwIhmgvMK58ckituwu5ZlRKu/IFNrBi2SAP/x7Ce8u2cwvLjmJk45VeUem0IAXibh3FhfzhzeWcFl2e65UeUdG0YAXibD1W/cwbmoh3duovCMTacCLRFTpvgrGTC6grNx5bFg2RzSsF3YkSTI9yCoSUZXlHY9enc1xKu/ISNrBi0TQS3M28MT7K7jmjM5c2PuYsONISDTgRSJmefFObpk+m34dW3Db+SrvyGQa8CIRsqe0nFG5BTSoZzxytco7Mp3O4EUipLK84y/fP0XlHaIdvEhUTPt4DU/lr2XsOSeovEMADXiRSJi3fht3PDeXM7u2Yty5J4QdR1JEkJV9HczsTTObb2bzzGxcUGuJZLJYeUcBLRs35KGr+qq8Qz4X5Bn8PuAmdy8ws2ZAvpm95u7zA1xTJKO4Ozc/NYt18fKOo1XeIVUEtoN39w3uXhB/ewewAGgX1Hoimejx91bwr3kbufX8HuSovEP2k5QzeDPrTKyfdeYBPjfCzPLMLK+4uDgZcUQiIW/lFu5+eSFfP6kNPzhT5R3yfwU+4M2sKfA0cKO7b9//8+4+0d1z3D0nKysr6DgikbB5517GTFZ5hxxaoM+DN7MGxIZ7rrvPCHItkUxRXuHcOLWIz3aXMmPU6TRvpPIOObDABrzFthSPAwvc/f6g1hHJNA/9ewnvLd3MPZf1VnmHHFKQRzRnAN8BzjGzovifCwJcTyTy3lq0iYffWMLl/dtzhco75EsEtoN39/cAHQyK1JH1W/cw/skiurdpxi8u7RV2HEkDeiWrSBoo3VfBaJV3SA3pYmMiaeC3Ly+gcPVWHhum8g6pPu3gRVLci7M38Jf3V3LtGV244GSVd0j1acCLpLBlxTu5Zfossju24Nbze4QdR9KMBrxIitpTWs6oSQUc3qCeyjskITqDF0lB7s7Pnp3L4k07+Ns1AzhW5R2SAG0JRFLQtLw1PF2wlhvOOYGzu+kSHpIYDXiRFBMr75jHWSe04gaVd0gtaMCLpJBte2LlHUc1bsiDV6q8Q2pHZ/AiKaJqeceT16u8Q2pPO3iRFPHnd1fw6vyN3HbBifTvpPIOqT0NeJEU8PHKLdz9ykLO79WWa8/oHHYciQgNeJGQxco7Cuh4VGPu+XZvlXdIndGAFwlReYUzbmohW3eX8diwbJqpvEPqkB5kFQnRQ68v5v2ln3LPt3tz4jHNw44jEaMdvEhI3ly0iT+8sZQrctpzRY7KO6TuBTbgzewJM9tkZnODWkMkXa2Ll3f0aKvyDglOkDv4vwJDArx/kbRUuq+CUbkFlJc7E4b3p1EDlXdIMAIb8O7+DrAlqPsXSVe/eWkBs9Zs5d7Le9OlVZOw40iEhX4Gb2YjzCzPzPKKi4vDjiMSqBdmr+ev/1nJD87swpBeKu+QYIU+4N19orvnuHtOVpaumifRtax4Jz+ZPpv+nVqqvEOSIvQBL5IJdpfuY+Sk/Hh5Rz8a1NN/ehI8PQ9eJGDuzs+emcuSTTv5+7UDOOZIlXdIcgT5NMkpwAdAdzNba2Y/CGotkVQ29eM1zChcx43nduOsE3QMKckT2A7e3YcGdd8i6WLuum3c+c95nN0ti7HndA07jmQYHQSKBKSyvOPoJrHyjsNU3iFJpjN4kQC4Oz9+ahbrt+7hyesHclSThmFHkgykHbxIAP707nJem7+R2y84kf6dWoYdRzKUBrxIHftoxRZ+98oiLji5LdeovENCpAEvUoeKd/y3vON3l6m8Q8KlAS9SR8ornBumFLK9pIwJw1XeIeHTg6wideSB1xbzwfJPue/yPvRoq/IOCZ928CJ14M1Fm3jkzaVcmdOBb/dvH3YcEUADXqTW1n62m/FPFnHiMc2569KTwo4j8jkNeJFa2LuvnNGTC2PlHcOyVd4hKUVn8CK18JsXY+Udfxzen84q75AUox28SIL+OWs9f/tgFded1YUhvdqGHUfk/9CAF0nA0k07ufXp2eR0asktQ1TeIalJA16khnaX7mNUbj5HNKjHI1dnq7xDUpbO4EVqwN35aby84x/XnkrbIxuFHUnkoLT1EKmBKR+t4ZnCdYwf3I0zT2gVdhyRQwp0wJvZEDNbZGZLzezWINcSCdqctdv4f/HyjjGDVN4hqS/Iyr56wKPA+UBPYKiZ9QxqPZEgbdtdxqjJ+bRqqvIOSR9BnsEPAJa6+3IAM5sKXArMr+uFLn74PUrKyuv6bkU+t21PGZ/tLlV5h6SVIAd8O2BNlffXAqfufyMzGwGMAOjYsWNCCx2f1YTS8oqEvlakui7pcyzZHVXeIekj9GfRuPtEYCJATk6OJ3IfD17Vr04ziYhEQZAPsq4DOlR5v338YyIikgRBDviPgRPMrIuZNQSuAv4Z4HoiIlJFYEc07r7PzMYA/wLqAU+4+7yg1hMRkS8K9Aze3V8CXgpyDREROTC9klVEJKI04EVEIkoDXkQkojTgRUQiytwTem1RIMysGFiV4Je3AjbXYZy6olw1l6rZlKvmUjVblHJ1cvesA30ipQZ8bZhZnrvnhJ1jf8pVc6maTblqLlWzZUouHdGIiESUBryISERFacBPDDvAQShXzaVqNuWquVTNlhG5InMGLyIiXxSlHbyIiFShAS8iElGRGvBm1tfMPjSzIjPLM7MBYWeqZGZjzWyhmc0zs3vCzlOVmd1kZm5mrcLOAmBm98b/rmab2TNm1iLkPClZHm9mHczsTTObH/++Ghd2pqrMrJ6ZFZrZC2FnqWRmLcxsevz7a4GZDQw7UyUzGx//d5xrZlPMrFFt7zNSAx64B7jL3fsCP4+/HzozG0Ssj7aPu58E3BdypM+ZWQfga8DqsLNU8RrQy917A4uB28IKkuLl8fuAm9y9J3AaMDqFsgGMAxaEHWI/DwGvuHsPoA8pks/M2gE3ADnu3ovYJdavqu39Rm3AO9A8/vaRwPoQs1Q1Erjb3fcCuPumkPNU9QBwC7G/u5Tg7q+6+774ux8SawMLy+fl8e5eClSWx4fO3Te4e0H87R3EhlW7cFPFmFl74ELgz2FnqWRmRwJnA48DuHupu28NN9UX1AeOMLP6QGPqYH5FbcDfCNxrZmuI7ZJD2/ntpxtwlpnNNLO3zeyUsAMBmNmlwDp3nxV2lkO4Fng5xPUPVB6fEkO0KjPrDPQDZoab5HMPEts4VIQdpIouQDHwl/jR0Z/NrEnYoQDcfR2xmbUa2ABsc/dXa3u/oZdu15SZvQ60PcCnfgqcC4x396fN7ApiP6kHp0Cu+sBRxH6NPgWYZmbHeRKeo/oluW4ndjyTdIfK5e7PxW/zU2LHELnJzJZuzKwp8DRwo7tvT4E8FwGb3D3fzL4adp4q6gPZwFh3n2lmDwG3AneEGwvMrCWx3wy7AFuBp8xsuLtPqs39pt2Ad/eDDmwz+zuxcz+Ap0jir4dfkmskMCM+0D8yswpiFxUqDiuXmZ1M7JtplplB7BikwMwGuPsnYeWqku/7wEXAucn4QXgIKV0eb2YNiA33XHefEXaeuDOAS8zsAqAR0NzMJrn78JBzrQXWunvlbznTiQ34VDAYWOHuxQBmNgM4HajVgI/aEc164Cvxt88BloSYpapngUEAZtYNaEjIV7Jz9znu3trdO7t7Z2Lf/NnJGO5fxsyGEPv1/hJ33x1ynJQtj7fYT+bHgQXufn/YeSq5+23u3j7+fXUV8EYKDHfi39trzKx7/EPnAvNDjFTVauA0M2sc/3c9lzp4ADjtdvBf4jrgofiDFCXAiJDzVHoCeMLM5gKlwPdC3pWmukeAw4HX4r9dfOjuPwojSIqXx58BfAeYY2ZF8Y/dHu9ClgMbC+TGf1gvB64JOQ8A8SOj6UABsWPJQurgsgW6VIGISERF7YhGRETiNOBFRCJKA15EJKI04EVEIkoDXkQkojTgRUQiSgNeRCSiNOBFDsLMTolfk76RmTWJX6u7V9i5RKpLL3QSOQQz+xWx66kcQew6Jr8NOZJItWnAixxC/CXtHxO79MXp7l4eciSRatMRjcihHQ00BZoR28mLpA3t4EUOwcz+SazFqQtwjLuPCTmSSLVF7WqSInXGzL4LlLn75Hg363/M7Bx3fyPsbCLVoR28iEhE6QxeRCSiNOBFRCJKA15EJKI04EVEIkoDXkQkojTgRUQiSgNeRCSi/j+Pc2VPvno+UAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAzWGZdeQHfJ"
      },
      "source": [
        "When the input is negative, the derivative of ReLU function is 0\n",
        "and when the input is positive, the derivative of ReLU function is 1.\n",
        "Note that the ReLU function is not differentiable\n",
        "when the input takes value precisely equal to  0.\n",
        "In these cases, we go with the left-hand-side (LHS) derivative\n",
        "and say that the derivative is 0 when the input is 0.\n",
        "We can get away with this because the input may never actually be zero.\n",
        "There's an old adage that if subtle boundary conditions matter,\n",
        "we are probably doing (*real*) mathematics, not engineering.\n",
        "That conventional wisdom may apply here.\n",
        "See the derivative of the ReLU function plotted below.\n",
        "\n",
        "When we use .backward(), by default it is .backward(torch.Tensor([1])).This is useful when we are dealing with single scalar input.But here we are dealing with a vector input so we have to use this snippet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNiVPTVjQHfJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ec11c053-2141-425a-c152-1ba151179317"
      },
      "source": [
        "y.backward(torch.ones_like(x), retain_graph=True)\n",
        "xyplot(x,x.grad,\"grad of relu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pry_-qSpQHfM"
      },
      "source": [
        "Note that there are many variants to the ReLU function, such as the parameterized ReLU (pReLU) of [He et al., 2015](https://arxiv.org/abs/1502.01852). This variation adds a linear term to the ReLU, so some information still gets through, even when the argument is negative.\n",
        "\n",
        "$$\\mathrm{pReLU}(x) = \\max(0, x) + \\alpha \\min(0, x)$$\n",
        "\n",
        "The reason for using the ReLU is that its derivatives are particularly well behaved - either they vanish or they just let the argument through. This makes optimization better behaved and it reduces the issue of the vanishing gradient problem (more on this later).\n",
        "\n",
        "## Sigmoid Function\n",
        "\n",
        "The sigmoid function transforms its inputs which take values in $\\mathbb{R}$ to the interval $(0,1)$.\n",
        "For that reason, the sigmoid is often called a *squashing* function:\n",
        "it squashes any input in the range (-inf, inf)\n",
        "to some value in the range (0,1).\n",
        "\n",
        "$$\\mathrm{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.$$\n",
        "\n",
        "In the earliest neural networks, scientists\n",
        "were interested in modeling biological neurons\n",
        "which either *fire* or *don't fire*.\n",
        "Thus the pioneers of this field, going all the way back to McCulloch and Pitts in the 1940s, were focused on thresholding units.\n",
        "A thresholding function takes either value $0$\n",
        "(if the input is below the threshold)\n",
        "or value $1$ (if the input exceeds the threshold)\n",
        "\n",
        "\n",
        "When attention shifted to gradient based learning,\n",
        "the sigmoid function was a natural choice\n",
        "because it is a smooth, differentiable approximation to a thresholding unit.\n",
        "Sigmoids are still common as activation functions on the output units,\n",
        "when we want to interpret the outputs as probabilities\n",
        "for binary classification problems\n",
        "(you can think of the sigmoid as a special case of the softmax)\n",
        "but the sigmoid has mostly been replaced by the simpler and easier to train ReLU for most use in hidden layers.\n",
        "In the \"Recurrent Neural Network\" chapter, we will describe\n",
        "how sigmoid units can be used to control\n",
        "the flow of information in a neural network\n",
        "thanks to its capacity to transform the value range between 0 and 1.\n",
        "\n",
        "See the sigmoid function plotted below.\n",
        "When the input is close to 0, the sigmoid function\n",
        "approaches a linear transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuN7nVQoQHfN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "fa83b1ff-91e1-48ba-b25e-551d784b901a"
      },
      "source": [
        "x = Variable(torch.arange(-8.0,8.0,0.1, dtype=torch.float32).reshape(int(16/0.1),1), requires_grad=True)\n",
        "y = torch.sigmoid(x)\n",
        "xyplot(x,y,'sigmoid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTyTY-_AQHfP"
      },
      "source": [
        "The derivative of sigmoid function is given by the following equation:\n",
        "\n",
        "$$\\frac{d}{dx} \\mathrm{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\mathrm{sigmoid}(x)\\left(1-\\mathrm{sigmoid}(x)\\right).$$\n",
        "\n",
        "\n",
        "The derivative of sigmoid function is plotted below.\n",
        "Note that when the input is 0, the derivative of the sigmoid function\n",
        "reaches a maximum of 0.25. As the input diverges from 0 in either direction, the derivative approaches 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR5YyvIZQHfQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "957d3950-cc56-41e4-910c-e76c4b309840"
      },
      "source": [
        "y.backward(torch.ones_like(x), retain_graph=True)\n",
        "xyplot(x,x.grad,'grad of sigmoid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQSTzchhQHfS"
      },
      "source": [
        "## Tanh Function\n",
        "\n",
        "Like the sigmoid function, the tanh (Hyperbolic Tangent)\n",
        "function also squashes its inputs,\n",
        "transforms them into elements on the interval between -1 and 1:\n",
        "\n",
        "$$\\text{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$\n",
        "\n",
        "We plot the tanh function blow. Note that as the input nears 0, the tanh function approaches a linear transformation. Although the shape of the function is similar to the sigmoid function, the tanh function exhibits point symmetry about the origin of the coordinate system.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFZxPHnQHfS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a9c3dea6-bbb1-4f2e-ced2-892e605fde12"
      },
      "source": [
        "x = Variable(torch.arange(-8.0,8.0,0.1, dtype=torch.float32).reshape(int(16/0.1),1), requires_grad=True)\n",
        "y = torch.tanh(x)\n",
        "xyplot(x,y,\"tanh\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFgSsaFgQ6yz"
      },
      "source": [
        "The derivative of the Tanh function is:\n",
        "\n",
        "$$\\frac{d}{dx} \\mathrm{tanh}(x) = 1 - \\mathrm{tanh}^2(x).$$\n",
        "\n",
        "The derivative of tanh function is plotted below.\n",
        "As the input nears 0,\n",
        "the derivative of the tanh function approaches a maximum of 1.\n",
        "And as we saw with the sigmoid function,\n",
        "as the input moves away from 0 in either direction,\n",
        "the derivative of the tanh function approaches 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASwhYKA2Q6y0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "0da3b34b-2e24-44da-98f5-599381683121"
      },
      "source": [
        "y.backward(torch.ones_like(x), retain_graph=True)\n",
        "xyplot(x,x.grad,\"grad of tanh\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCfIunp_Q6y2"
      },
      "source": [
        "In summary, we now know how to incorporate nonlinearities\n",
        "to build expressive multilayer neural network architectures.\n",
        "As a side note, your knowledge now already\n",
        "puts you in command of the state of the art in deep learning, circa 1990.\n",
        "In fact, you have an advantage over anyone working the 1990s,\n",
        "because you can leverage powerful open-source deep learning frameworks\n",
        "to build models rapidly, using only a few lines of code.\n",
        "Previously, getting these nets training\n",
        "required researchers to code up thousands of lines of C and Fortran.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jE8o5-ssq8S"
      },
      "source": [
        "# Model Selection, Underfitting and Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz_3I3ihshGz"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "In machine learning, we usually select our final model\n",
        "after evaluating several candidate models.\n",
        "This process is called model selection.\n",
        "Sometimes the models subject to comparison\n",
        "are fundamentally different in nature\n",
        "(say, decision trees vs linear models).\n",
        "At other times, we are comparing\n",
        "members of the same class of models\n",
        "that have been trained with different hyperparameter settings.\n",
        "\n",
        "With multilayer perceptrons for example,\n",
        "we may wish to compare models with\n",
        "different numbers of hidden layers,\n",
        "different numbers of hidden units,\n",
        "and various choices of the activation functions\n",
        "applied to each hidden layer.\n",
        "In order to determine the best among our candidate models,\n",
        "we will typically employ a validation set.\n",
        "\n",
        "\n",
        "### Validation Dataset\n",
        "\n",
        "In principle we should not touch our test set\n",
        "until after we have chosen all our hyper-parameters.\n",
        "Were we to use the test data in the model selection process,\n",
        "there is a risk that we might overfit the test data.\n",
        "Then we would be in serious trouble.\n",
        "If we overfit our training data,\n",
        "there is always the evaluation on test data to keep us honest.\n",
        "But if we overfit the test data, how would we ever know?\n",
        "\n",
        "\n",
        "Thus, we should never rely on the test data for model selection.\n",
        "And yet we cannot rely solely on the training data\n",
        "for model selection either because\n",
        "we cannot estimate the generalization error\n",
        "on the very data that we use to train the model.\n",
        "\n",
        "The common practice to address this problem\n",
        "is to split our data three ways,\n",
        "incorporating a *validation set*\n",
        "in addition to the training and test sets.\n",
        "\n",
        "\n",
        "In practical applications, the picture gets muddier.\n",
        "While ideally we would only touch the test data once,\n",
        "to assess the very best model or to compare\n",
        "a small number of models to each other,\n",
        "real-world test data is seldom discarded after just one use.\n",
        "We can seldom afford a new test set for each round of experiments.\n",
        "\n",
        "The result is a murky practice where the boundaries\n",
        "between validation and test data are worryingly ambiguous.\n",
        "Unless explicitly stated otherwise, in the experiments in this book\n",
        "we are really working with what should rightly be called\n",
        "training data and validation data, with no true test sets.\n",
        "Therefore, the accuracy reported in each experiment\n",
        "is really the validation accuracy and not a true test set accuracy.\n",
        "The good news is that we do not need too much data in the validation set.\n",
        "The uncertainty in our estimates can be shown\n",
        "to be of the order of $\\mathcal{O}(n^{-\\frac{1}{2}})$.\n",
        "\n",
        "\n",
        "### $K$-Fold Cross-Validation\n",
        "\n",
        "When training data is scarce,\n",
        "we might not even be able to afford to hold out\n",
        "enough data to constitute a proper validation set.\n",
        "One popular solution to this problem is to employ\n",
        "$K$*-fold cross-validation*.\n",
        "Here, the original training data is split into $K$ non-overlapping subsets.\n",
        "Then model training and validation are executed $K$ times,\n",
        "each time training on $K-1$ subsets and validating\n",
        "on a different subset (the one not used for training in that round).\n",
        "Finally, the training and validation error rates are estimated\n",
        "by averaging over the results from the $K$ experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yYdUa3W9kI2"
      },
      "source": [
        "**Exercise:** Implement a 5-fold validation scheme using the training data set from FashionMNIST. The function below should return the *average* accuracy over all 5 folds. Note: Use the same network, parameters, hyperparameters, and data from above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc3TAzce9zKl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de2dc144-d835-4856-a3e8-4735947af854"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0.0, 0\n",
        "    for X, y in data_iter:\n",
        "        acc_sum += (net(X).argmax(dim=1) == y).sum().item()\n",
        "        n += y.size()[0]  \n",
        "    return acc_sum / n\n",
        "\n",
        "def five_fold_validation(train_iter, test_iter):\n",
        "  accuracy_fold = []\n",
        "  avg_accuracy = 0.0\n",
        "  K = 5   # number of folds\n",
        "  ## Write your code here\n",
        "\n",
        "  # getting mnist_train\n",
        "  trans = transforms.ToTensor() \n",
        "  mnist_train = torchvision.datasets.FashionMNIST(root=\"./\", train=True, transform=trans, target_transform=None, download=True)\n",
        "  \n",
        "\n",
        "  # splitting the mnist_train dataset to K=5 train/test_sets \n",
        "\n",
        "  kf = KFold(n_splits = K, shuffle = True)\n",
        "\n",
        "  for fold, (train_index, test_index) in enumerate(kf.split(mnist_train)):\n",
        "    mnist_train_fold = torch.utils.data.Subset(mnist_train, train_index)\n",
        "    mnist_test_fold = torch.utils.data.Subset(mnist_train, test_index)\n",
        "  \n",
        "  # preparing batches of our new and setting hyperparameters\n",
        "  \n",
        "    lr, num_epochs, batch_size = 0.5, 10, 256\n",
        "    \n",
        "    if sys.platform.startswith('win'):\n",
        "      num_workers = 0\n",
        "    else:\n",
        "      num_workers = 4\n",
        "\n",
        "    train_iter_fold = DataLoader(mnist_train_fold, batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_iter_fold = DataLoader(mnist_test_fold, batch_size, shuffle=False, num_workers=num_workers)\n",
        "  \n",
        "  # initalise model with my changed Net-Class and hyperparameters:\n",
        "    net = Net(activator_function = nn.ReLU())\n",
        "    net.apply(init_weights)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)\n",
        "  # train\n",
        "    for epoch in range(num_epochs):\n",
        "      net.train()\n",
        "      for X, y in train_iter_fold:\n",
        "        y_hat = net(X)\n",
        "        l = criterion(y_hat, y)\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # accuracy for #fold:\n",
        "    accuracy_fold.append(evaluate_accuracy(test_iter_fold,net))\n",
        "\n",
        "  avg_accuracy = sum(accuracy_fold)/len(accuracy_fold)\n",
        "  ## end of function\n",
        "  return avg_accuracy\n",
        "\n",
        "five_fold_validation(train_iter, test_iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.86745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H67vehrf9fNs"
      },
      "source": [
        "## Underfitting or Overfitting?\n",
        "When we compare the training and validation errors, we want to be mindful of two common situations: First, we want\n",
        "to watch out for cases when our training error and validation error are both substantial but there is a little gap between\n",
        "them. If the model is unable to reduce the training error, that could mean that our model is too simple (i.e., insufficiently\n",
        "expressive) to capture the pattern that we are trying to model. Moreover, since the generalization gap between our training\n",
        "and validation errors is small, we have reason to believe that we could get away with a more complex model. This\n",
        "phenomenon is known as underfitting.\n",
        "\n",
        "On the other hand, as we discussed above, we want to watch out for the cases when our training error is significantly lower\n",
        "than our validation error, indicating severe overfitting. Note that overfitting is not always a bad thing. With deep learning\n",
        "especially, it’s well known that the best predictive models often perform far better on training data than on holdout data.\n",
        "\n",
        "Ultimately, we usually care more about the validation error than about the gap between the training and validation errors.\n",
        "Whether we overfit or underfit can depend both on the complexity of our model and the size of the available training\n",
        "datasets, two topics that we discuss below.\n",
        "\n",
        "## Model Complexity \n",
        "\n",
        "To illustrate some classical intuition about overfitting and model complexity, we given an example using polynomials.\n",
        "Given training data consisting of a single feature x and a corresponding real-valued label y, we try to find the polynomial\n",
        "of degree d\n",
        "\n",
        "$$y=\\sum_{i=0}^d\\ W^ix^i$$\n",
        "\n",
        "to estimate the labels y. This is just a linear regression problem where our features are given by the powers of x, the wi\n",
        "given the model’s weights, and the bias is given by w0 since x\n",
        "0 = 1 for all x. Since this is just a linear regression problem,\n",
        "we can use the squared error as our loss function.\n",
        "A higher-order polynomial function is more complex than a lower order polynomial function, since the higher-order polynomial has more parameters and the model function’s selection range is wider. Fixing the training data set, higher-order\n",
        "polynomial functions should always achieve lower (at worst, equal) training error relative to lower degree polynomials.\n",
        "In fact, whenever the data points each have a distinct value of x, a polynomial function with degree equal to the number\n",
        "of data points can fit the training set perfectly. We visualize the relationship between polynomial degree and under- vs\n",
        "over-fitting below.\n",
        "\n",
        "## Data Set Size\n",
        "\n",
        "The other big consideration to bear in mind is the dataset size. Fixing our model, the fewer samples we have in the\n",
        "training dataset, the more likely (and more severely) we are to encounter overfitting. As we increase the amount of\n",
        "training data, the generalization error typically decreases. Moreover, in general, more data never hurts. For a fixed task\n",
        "and data distribution, there is typically a relationship between model complexity and dataset size. Given more data, we\n",
        "might profitably attempt to fit a more complex model. Absent sufficient data, simpler models may be difficult to beat. For\n",
        "many tasks, deep learning only outperforms linear models when many thousands of training examples are available. In part, the current success of deep learning owes to the current abundance of massive datasets due to internet companies,\n",
        "cheap storage, connected devices, and the broad digitization of the economy.\n",
        "\n",
        "## Polynomial Regression\n",
        "We can now explore these concepts interactively by fitting polynomials to data. To get started we’ll import our usual\n",
        "packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "973_Ufhz9fNt"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIes1em49fNw"
      },
      "source": [
        "### Generating Data Sets\n",
        "\n",
        "First we need data. Given x, we will use the following cubic polynomial to generate the labels on training and test data:\n",
        "\n",
        "$$y=5+1.2x-3.4\\frac{x^2}{2!}+5.6\\frac{x^3}{3!}+ E where E-N(0,0.1)$$\n",
        "\n",
        "The noise term ϵ obeys a normal distribution with a mean of 0 and a standard deviation of 0.1. We’ll synthesize 100\n",
        "samples each for the training set and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFY-xoDc9fNw"
      },
      "source": [
        "max_degree=20\n",
        "n_train,n_test=100,100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fai0nHcw9fNy"
      },
      "source": [
        "poly_features=torch.zeros(20,200)\n",
        "true_w=torch.zeros(max_degree)\n",
        "true_w[0:4] = torch.tensor([5, 1.2, -3.4, 5.6])\n",
        "features = torch.randn(size=(n_train + n_test, 1))\n",
        "x_list=torch.arange(max_degree)\n",
        "x_list.float()\n",
        "features=features.reshape(1,-1)\n",
        "\n",
        "for i in range(1,max_degree):\n",
        "    \n",
        "    poly_features[i] = torch.pow(features,i)\n",
        "    \n",
        "print(features[:,4])\n",
        "print(poly_features[:,4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTPi2hwn9fN1"
      },
      "source": [
        "For optimization, we typically want to avoid very large values of gradients, losses, etc. This is why the monomials stored\n",
        "in poly_features are rescaled from x\n",
        "\n",
        "It allows us to avoid very large values for large exponents i. Factorials\n",
        "are implemented in Gluon using the Gamma function, where n! = Γ(n + b 1).\n",
        "Take a look at the first 2 samples from the generated data set. The value 1 is technically a feature, namely the constant\n",
        "feature corresponding to the bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNjqZOGw9fN2"
      },
      "source": [
        "from scipy.special import factorial\n",
        "ok=torch.arange(1,(max_degree) + 1).reshape((1, -1))\n",
        "\n",
        "dr=factorial(ok)\n",
        "\n",
        "dr2=torch.from_numpy(dr)\n",
        "\n",
        "poly_features = poly_features.double() /dr2.t()\n",
        "\n",
        "labels = torch.matmul(true_w.double(),poly_features)\n",
        "\n",
        "poly_features = poly_features.type(torch.FloatTensor)\n",
        "\n",
        "labels = labels.type(torch.FloatTensor)\n",
        "\n",
        "labels += torch.randn(200)*0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPxGr6ek9fN4"
      },
      "source": [
        "### Defining, Training and Testing Model\n",
        "We first define the plotting functionsemilogy, where the y axis makes use of the logarithmic scale\n",
        "\n",
        "Since we will be attempting to fit the generated dataset using models of varying complexity, we insert the model definition\n",
        "into the fit_and_plot function. The training and testing steps involved in polynomial function fitting are similar to\n",
        "those previously described in softmax regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw1ZyFfu9fN4"
      },
      "source": [
        "def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None, legend=None, figsize=(3.5, 2.5)):    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.semilogy(x_vals, y_vals)\n",
        "    if x2_vals and y2_vals:\n",
        "        plt.semilogy(x2_vals, y2_vals, linestyle=':')\n",
        "        plt.legend(legend)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i-GZMV29fN6"
      },
      "source": [
        "def fit_and_plot(train_features,train_labels,test_features,test_labels,no_inputs):\n",
        "    class LinearRegressionModel(torch.nn.Module): \n",
        "        def __init__(self): \n",
        "            super(LinearRegressionModel, self).__init__() \n",
        "            self.linear = torch.nn.Linear(no_inputs, 1)  \n",
        "  \n",
        "        def forward(self, x): \n",
        "            y_pred = self.linear(x) \n",
        "            return y_pred \n",
        "    \n",
        "    model = LinearRegressionModel() \n",
        "    criterion = torch.nn.MSELoss(reduction='sum') \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "    train_ls,test_ls=[],[]   \n",
        "    train_labels=train_labels.reshape(-1,1)\n",
        "    train_ds=TensorDataset(train_features,train_labels)\n",
        "    batch_size=10\n",
        "    train_dl=DataLoader(train_ds,batch_size,shuffle=True)    \n",
        "    test_labels=test_labels.reshape(-1,1)\n",
        "    for ep in range(100):\n",
        "        for xb,yb in train_dl:         \n",
        "            pred_y = model(xb)     \n",
        "            loss = criterion(pred_y, yb)             \n",
        "            optimizer.zero_grad() \n",
        "            loss.backward() \n",
        "            optimizer.step()\n",
        "          \n",
        "        predytr=model(train_features)\n",
        "        train_ls.append((criterion(predytr,train_labels)).mean())\n",
        "        predyts=model(test_features)\n",
        "        test_ls.append((criterion(predyts,test_labels)).mean())    \n",
        "    \n",
        "    print('final epoch:train loss',train_ls[-1],'test Loss',test_ls[-1])\n",
        "    semilogy(range(1,ep+2), train_ls,'epoch','loss',range(1,ep+2),test_ls,['train','test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kja7I86v9fN9"
      },
      "source": [
        "### Third-order Polynomial Function Fitting (Normal)\n",
        "\n",
        "We will begin by first using a third-order polynomial function with the same order as the data generation function. The\n",
        "results show that this model’s training error rate when using the testing data set is low. The trained model parameters are\n",
        "also close to the true values w = [5, 1.2, −3.4, 5.6].\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7OcI-OQ9fN9"
      },
      "source": [
        "poly_features_t=poly_features.t()\n",
        "fit_and_plot(train_features=poly_features_t[:100,0:4],train_labels=labels[:100],test_features=poly_features_t[100:,0:4],test_labels=labels[100:],no_inputs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKaBc4WJ9fOA"
      },
      "source": [
        "### Linear Function Fitting\n",
        "Let’s take another look at linear function fitting. After the decline in the early epoch, it becomes difficult to further\n",
        "decrease this model’s training error rate. After the last epoch iteration has been completed, the training error rate is\n",
        "still high. When used to fit non-linear patterns (like the third-order polynomial function here) linear models are liable to\n",
        "underfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "na-5Vv3-9fOA"
      },
      "source": [
        "fit_and_plot(train_features=poly_features_t[:100,0:3],train_labels=labels[:100],test_features=poly_features_t[100:,0:3],test_labels=labels[100:],no_inputs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTyiIuGa9fOC"
      },
      "source": [
        "### Insufficient Training(Overfitting)\n",
        "Now let’s try to train the model using a polynomial of too high degree. Here, there is insufficient data to learn that the\n",
        "higher-degree coefficients should have values close to zero. As a result, our overly-complex model is far too susceptible\n",
        "to being influenced by noise in the training data. Of course, our training error will now be low (even lower than if we had\n",
        "the right model!) but our test error will be high.\n",
        "Try out different model complexities (n_degree) and training set sizes (n_subset) to gain some intuition of what is\n",
        "happening.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nElQOGu9fOC"
      },
      "source": [
        "fit_and_plot(train_features=poly_features_t[1:100,0:20],train_labels=labels[1:100],test_features=poly_features_t[100:,0:20],test_labels=labels[100:],no_inputs=20)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}